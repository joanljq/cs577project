{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install torch\n",
    "# pip install gensim\n",
    "# pip install pandas\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "\n",
    "import argparse\n",
    "import random\n",
    "random.seed(577)\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(577)\n",
    "\n",
    "import torch\n",
    "torch.set_default_tensor_type(torch.FloatTensor)\n",
    "torch.use_deterministic_algorithms(True)\n",
    "torch.manual_seed(577)\n",
    "torch_device = torch.device(\"cpu\")\n",
    "\n",
    "'''\n",
    "NOTE: Do not change any of the statements above regarding random/numpy/pytorch.\n",
    "You can import other built-in libraries (e.g. collections) or pre-specified external libraries\n",
    "such as pandas, nltk and gensim below. \n",
    "Also, if you'd like to declare some helper functions, please do so in utils.py and\n",
    "change the last import statement below.\n",
    "'''\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch import tensor\n",
    "\n",
    "import gensim.downloader as api\n",
    "\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "# from neural_archs import DAN, RNN, LSTM\n",
    "# from utils import WiCDataset, sen2vec, sen2glove\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     \\\\nas01.itap.purdue.edu\\puhome\\ecn.data\\bad\\nltk_data.\n",
      "[nltk_data]     ..\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     \\\\nas01.itap.purdue.edu\\puhome\\ecn.data\\bad\\nltk_data.\n",
      "[nltk_data]     ..\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package tagsets to\n",
      "[nltk_data]     \\\\nas01.itap.purdue.edu\\puhome\\ecn.data\\bad\\nltk_data.\n",
      "[nltk_data]     ..\n",
      "[nltk_data]   Package tagsets is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# from utils.py\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch import tensor\n",
    "\n",
    "\n",
    "import nltk\n",
    "# import tempfile\n",
    "#\n",
    "# nltk.download('punkt', download_dir=tempfile.gettempdir())\n",
    "# nltk.download('averaged_perceptron_tagger', download_dir=tempfile.gettempdir())\n",
    "# nltk.download('tagsets', download_dir=tempfile.gettempdir())\n",
    "# nltk.data.path.append(tempfile.gettempdir())\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('tagsets')\n",
    "\n",
    "from nltk.data import load\n",
    "paras = load('taggers/averaged_perceptron_tagger/averaged_perceptron_tagger.pickle')\n",
    "\n",
    "from nltk import pos_tag\n",
    "\n",
    "ss = 'NN NNS NNP NNPS VB VBD VBG VBN VBP VBZ JJ JJR JJS RBR BR RBS WDT WP WP$ PRP PRP$ DT CD UH SYM FW  LS'\n",
    "\n",
    "selected_tags = ss.split()\n",
    "pos_dict = {}\n",
    "i = 0\n",
    "for pos in selected_tags:\n",
    "    pos_dict[pos] = i\n",
    "    i += 1\n",
    "n = len(pos_dict)\n",
    "\n",
    "# https://pytorch.org/tutorials/beginner/data_loading_tutorial.html\n",
    "class WiCDataset(Dataset):\n",
    "\n",
    "    def __init__(self, mode='train', root_dir='./WiC_dataset/'):\n",
    "\n",
    "        self.root_dir = root_dir\n",
    "        self.mode = mode\n",
    "\n",
    "        file_name = root_dir + self.mode + '/'+ self.mode+'.data.txt'\n",
    "        with open(file_name,'r', encoding = 'cp850') as file:    \n",
    "            self.data = file.readlines()\n",
    "           \n",
    "        file_name = root_dir + self.mode + '/'+ self.mode +'.gold.txt'\n",
    "        with open(file_name,'r', encoding = 'cp850') as file:\n",
    "            self.labels = file.readlines()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "        \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        line = self.data[idx]\n",
    "        sample = {}\n",
    "        parts = line.replace(\"\\n\",\"\\t\").strip().split(\"\\t\")\n",
    "        sample['word'] = parts[0]\n",
    "\n",
    "        if parts[1] == \"F\":\n",
    "            sample['label'] = False\n",
    "        else:\n",
    "            sample['label'] = True\n",
    "\n",
    "        sample['sentence1'] = parts[3]\n",
    "        sample['sentence2'] = parts[4]\n",
    "\n",
    "        idxs = parts[2].split('-')\n",
    "        sample['idx1'] = idxs[0]\n",
    "        sample['idx2'] = idxs[1]\n",
    "\n",
    "\n",
    "        line = self.labels[idx]\n",
    "        if line.split()[0] == 'F':\n",
    "            sample['label'] = False\n",
    "        else:\n",
    "            sample['label'] = True\n",
    "\n",
    "        return sample\n",
    "\n",
    "    def get_vocab(self):  \n",
    "        vocab = {\"<UNK>\":0}\n",
    "        for line in self.data:\n",
    "            parts = line.replace(\"\\n\",\"\\t\").strip().split(\"\\t\")\n",
    "\n",
    "            #create vocabulary from all unique words in all sentences\n",
    "            sentence = parts[3] + \" \" + parts[4]\n",
    "            words = sentence.replace(\"'s\",\"\").lower().split()\n",
    "            #add if not already in vocab\n",
    "            for word in words:\n",
    "                if word not in vocab:\n",
    "                    #add word to vocab dict\n",
    "                    vocab[word] = len(vocab)\n",
    "        return vocab,len(vocab)\n",
    "    \n",
    "\n",
    "\n",
    "def sen2vec(s,vocab):\n",
    "    v = []\n",
    "    words = s.replace(\"'s\",\"\").lower().split()\n",
    "    for word in words:\n",
    "        try:\n",
    "            v.append(vocab[word])\n",
    "        except:\n",
    "            v.append(vocab[\"<UNK>\"])\n",
    "    return tensor(v).unsqueeze(0)\n",
    "\n",
    "\n",
    "def sen2glove(s,glove_embs):\n",
    "    v = []\n",
    "    words = s.replace(\"'s\",\"\").lower().split()\n",
    "    for word in words:\n",
    "        try:\n",
    "            v.append(glove_embs.get_index(word, default=None))\n",
    "        except:\n",
    "            v.append(40000)\n",
    "    return tensor(v).unsqueeze(0)\n",
    "\n",
    "def sen2pos(s,pos_dict):\n",
    "    v = []\n",
    "    words = s.replace(\"'s\",\"\").lower().split()\n",
    "    pos_tags = pos_tag(words)\n",
    "\n",
    "    for word, pos in pos_tags:\n",
    "        if pos in pos_dict.keys():\n",
    "            embed = np.zeros((n,), dtype=np.float32)\n",
    "            embed[pos_dict[pos]]=1\n",
    "        else:\n",
    "            # print(pos)\n",
    "            embed = np.zeros((n,), dtype=np.float32)\n",
    "\n",
    "        v.append(embed)\n",
    "    return tensor(np.array(v)).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_word_embs = 'glove'\n",
    "neural_arch = 'lstm'\n",
    "rnn_bidirect = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if init_word_embs == \"glove\":\n",
    "    # TODO: Feed the GloVe embeddings to NN modules appropriately\n",
    "    # for initializing the embeddings\n",
    "    glove_embs = api.load(\"glove-wiki-gigaword-50\")\n",
    "    all_weights = glove_embs.get_normed_vectors()\n",
    "    avg_wegihts = np.mean(all_weights,axis=0)\n",
    "    update_weights = np.vstack((all_weights,avg_wegihts))\n",
    "    weights = torch.FloatTensor(update_weights)\n",
    "else:\n",
    "    # vocab size is 7459 based on experiment\n",
    "    weights = torch.FloatTensor(np.random.rand(7459, 50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Read off the WiC dataset files from the `WiC_dataset' directory\n",
    "# (will be located in /homes/cs577/WiC_dataset/(train, dev, test))\n",
    "# and initialize PyTorch dataloader appropriately\n",
    "# Take a look at this page\n",
    "# https://pytorch.org/tutorials/beginner/data_loading_tutorial.html\n",
    "# and implement a PyTorch Dataset class for the WiC dataset in\n",
    "# utils.py\n",
    "root = 'D:/OneDrive - purdue.edu/Courses/CS577_NLP/hw/hw2/WiC_dataset/'\n",
    "train_data = WiCDataset(root_dir=root)\n",
    "vocab, _ = train_data.get_vocab()\n",
    "\n",
    "test_data = WiCDataset('test',root_dir=root)\n",
    "dev_data = WiCDataset('dev',root_dir=root)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(torch.nn.Module):\n",
    "    def __init__(self, rnn_bidirect, glove, load_weights, lstm_dim=10, layer_num=1, hidden_dim = 20,p_drop=0.1):\n",
    "\n",
    "        # TODO: Declare LSTM model architecture\n",
    "        super(LSTM, self).__init__()\n",
    "\n",
    "        if glove == 'glove':\n",
    "            self.embedding = nn.Embedding.from_pretrained(load_weights, freeze=True)\n",
    "        else:\n",
    "            self.embedding = nn.Embedding.from_pretrained(load_weights, freeze=False)\n",
    "\n",
    "        if rnn_bidirect:\n",
    "            self.lstm = nn.LSTM(50, lstm_dim, num_layers=layer_num, bias=False, bidirectional = True) # one layer, bidirectional\n",
    "            self.hidden_layer = nn.Linear(4*lstm_dim,hidden_dim)\n",
    "        else:\n",
    "            self.lstm = nn.LSTM(50, lstm_dim, num_layers=layer_num, bias=False) # two layers\n",
    "            self.hidden_layer = nn.Linear(2*lstm_dim,hidden_dim)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=p_drop)\n",
    "        self.output_layer = nn.Linear(hidden_dim, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, s1, s2):\n",
    "        # TODO: Implement LSTM forward pass\n",
    "        embed1 = self.embedding(s1).squeeze(0)\n",
    "\t\t# print(\"embedding_squeeze\",self.embedding(s1).squeeze(0).shape)\n",
    "        lstm_out1,(_,_) = self.lstm(embed1)\n",
    "\n",
    "        embed2 = self.embedding(s2).squeeze(0)\n",
    "        lstm_out2,(_,_) = self.lstm(embed2)\n",
    "        cat_rep = torch.cat((lstm_out1[-1,:].unsqueeze(0), lstm_out2[-1,:].unsqueeze(0)),1)\n",
    "\n",
    "        hidden_rep = self.hidden_layer(cat_rep)\n",
    "        relu_rep = self.relu(hidden_rep)\n",
    "\t\t# print(\"hidden\",hidden_rep.shape)\n",
    "        drop = self.dropout(relu_rep)\n",
    "\t\t# print(\"drop\",drop.shape)\n",
    "        output = self.output_layer(drop)\n",
    "        # print(\"ouput\",output.shape)\n",
    "\n",
    "        output = self.sigmoid(output)\n",
    "        # print(\"sigmoid\",output.shape)\n",
    "\n",
    "        return output.squeeze(0).squeeze(0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Training and validation loop here\n",
    "\n",
    "lr0 = 0.001\n",
    "epochs = 20\n",
    "patience = 5\n",
    "\n",
    "def training(model,train_dataset, valid_dataset,lr0,epochs):\n",
    "    ce = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr0)\n",
    "\n",
    "    train_acc = []\n",
    "    dev_acc = []\n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "    # test_output = []\n",
    "\n",
    "    best_val_loss = 1\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        #print(\"Epoch:\",i)\n",
    "        total_loss = 0\n",
    "\n",
    "        for i in range(len(train_dataset)):\n",
    "            sample = train_dataset[i]\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # a) calculate probs / get an output\n",
    "            if init_word_embs == \"glove\":\n",
    "                s1 = sen2glove(sample[\"sentence1\"],glove_embs)\n",
    "                s2 = sen2glove(sample[\"sentence2\"],glove_embs)\n",
    "            else:\n",
    "                s1 = sen2vec(sample[\"sentence1\"])\n",
    "                s2 = sen2vec(sample[\"sentence2\"])\n",
    "\n",
    "            y_raw = model(s1,s2)\n",
    "\n",
    "            y = tensor(float(sample[\"label\"]))\n",
    "            \n",
    "            # b) compute loss\n",
    "            loss = ce(y_raw,y)\n",
    "            total_loss += loss\n",
    "\n",
    "            # c) get the gradient\n",
    "            loss.backward()\n",
    "\n",
    "            # d) update the weights\n",
    "            optimizer.step()\n",
    "        train_loss.append(total_loss.item()/len(train_dataset))\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        score = 0\n",
    "        \n",
    "        for i in range(len(train_dataset)):\n",
    "            sample = train_dataset[i]\n",
    "\n",
    "            # a) calculate probs / get an output\n",
    "            if init_word_embs == \"glove\":\n",
    "                s1 = sen2glove(sample[\"sentence1\"],glove_embs)\n",
    "                s2 = sen2glove(sample[\"sentence2\"],glove_embs)\n",
    "            else:\n",
    "                s1 = sen2vec(sample[\"sentence1\"])\n",
    "                s2 = sen2vec(sample[\"sentence2\"])\n",
    "\n",
    "            y_raw = model(s1,s2)\n",
    "\n",
    "            result = True if y_raw >= 0.5 else False\n",
    "\n",
    "            if bool(result) == sample[\"label\"]:\n",
    "                score += 1\n",
    "        \n",
    "        train_acc.append(score/len(train_dataset))\n",
    "        \n",
    "\n",
    "        score = 0\n",
    "        total_loss = 0\n",
    "        for i in range(len(valid_dataset)):\n",
    "            sample = valid_dataset[i]\n",
    "            # a) calculate probs / get an output\n",
    "            if init_word_embs == \"glove\":\n",
    "                s1 = sen2glove(sample[\"sentence1\"],glove_embs)\n",
    "                s2 = sen2glove(sample[\"sentence2\"],glove_embs)\n",
    "            else:\n",
    "                s1 = sen2vec(sample[\"sentence1\"])\n",
    "                s2 = sen2vec(sample[\"sentence2\"])\n",
    "\n",
    "            y_raw = model(s1,s2)\n",
    "            \n",
    "            y = tensor(float(sample[\"label\"]))\n",
    "            loss = ce(y_raw,y)\n",
    "            total_loss += loss\n",
    "\n",
    "            result = True if y_raw >= 0.5 else False\n",
    "            if bool(result) == sample[\"label\"]:\n",
    "                score += 1\n",
    "\n",
    "        val_loss.append(total_loss.item()/len(valid_dataset))\n",
    "        dev_acc.append(score/len(valid_dataset))\n",
    "\n",
    "\n",
    "\n",
    "        if epoch% 10 == 0:\n",
    "            print(\"---------epoch:\",epoch,\"---------\")\n",
    "            print(\" train loss \", train_loss[-1]) \n",
    "            print(\" val loss \", val_loss[-1])\n",
    "            print(\" train accuracy \",train_acc[-1])\n",
    "            print(\" val accuracy \",dev_acc[-1])\n",
    "\n",
    "        # check if validation loss has improvedval loss < best val loss:\n",
    "        if val_loss[-1] < best_val_loss:\n",
    "            best_val_loss = val_loss[-1] \n",
    "            early_stop_counter = 0\n",
    "        else:\n",
    "            early_stop_counter += 1\n",
    "            if early_stop_counter >= patience:\n",
    "                print(\"------Early stopping after epoch:\",epoch,\"---------\")\n",
    "                print(\" train loss \", train_loss[-1]) \n",
    "                print(\" val loss \", val_loss[-1]) \n",
    "                print(\" train accuracy \",train_acc[-1])\n",
    "                print(\" val accuracy \",dev_acc[-1])\n",
    "                return train_loss,val_loss, train_acc,dev_acc\n",
    "\n",
    "\n",
    "    print(\"---------endng epoch:\",epoch,\"---------\")\n",
    "    print(\" train loss \", train_loss[-1]) \n",
    "    print(\" val loss \", val_loss[-1]) \n",
    "    print(\" train accuracy \",train_acc[-1])\n",
    "    print(\" val accuracy \",dev_acc[-1])\n",
    "    return train_loss,val_loss, train_acc,dev_acc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------epoch: 0 ---------\n",
      " train loss  0.6838354168345846\n",
      " val loss  0.6998994641916879\n",
      " train accuracy  0.5315033161385408\n",
      " val accuracy  0.5156739811912225\n",
      "------Early stopping after epoch: 6 ---------\n",
      " train loss  0.6512916694253178\n",
      " val loss  0.7083322172254605\n",
      " train accuracy  0.6000368459837878\n",
      " val accuracy  0.5376175548589341\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6000368459837878"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr0 = 0.001\n",
    "epochs = 20\n",
    "model = LSTM(rnn_bidirect,init_word_embs, weights).to(torch_device)\n",
    "train_loss,val_loss,train_acc,dev_acc = training(model,train_data, dev_data,lr0,epochs)\n",
    "train_acc[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTM_kFold(k,epochs,lr0,*inputs):\n",
    "    \n",
    "    num_val_samples = len(train_data)//k\n",
    "    cv_score = []\n",
    "    for i in range(k):\n",
    "        print('Processing fold: ', i + 1)\n",
    "        \"\"\"%%%% Initiate new model %%%%\"\"\" #in every fold\n",
    "        model = LSTM(*inputs).to(torch_device)\n",
    "        \n",
    "        valid_idx = np.arange(len(train_data))[i * num_val_samples:(i + 1) * num_val_samples]\n",
    "        train_idx = np.concatenate([np.arange(len(train_data))[:i * num_val_samples], np.arange(len(train_data))[(i + 1) * num_val_samples:]], axis=0)\n",
    "        \n",
    "        train_dataset = Subset(train_data, train_idx)\n",
    "        valid_dataset = Subset(train_data, valid_idx)\n",
    "\n",
    "        \n",
    "        _,_,_,valid_acc = training(model,train_dataset, valid_dataset,lr0,epochs)\n",
    "        cv_score.append(valid_acc[-1])\n",
    "    \n",
    "    print('cv_score: ',sum(cv_score)/len(cv_score))\n",
    "\n",
    "    return sum(cv_score)/len(cv_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing fold:  1\n",
      "---------epoch: 0 ---------\n",
      " train loss  0.6884259130173556\n",
      " val loss  0.7244971490675404\n",
      " train accuracy  0.5261340087497122\n",
      " val accuracy  0.4009216589861751\n",
      "---------epoch: 10 ---------\n",
      " train loss  0.6305356905149091\n",
      " val loss  0.7261966828377017\n",
      " train accuracy  0.6419525673497583\n",
      " val accuracy  0.5216589861751152\n",
      "------Early stopping after epoch: 10 ---------\n",
      " train loss  0.6305356905149091\n",
      " val loss  0.7261966828377017\n",
      " train accuracy  0.6419525673497583\n",
      " val accuracy  0.5216589861751152\n",
      "Processing fold:  2\n",
      "---------epoch: 0 ---------\n",
      " train loss  0.6859285730989524\n",
      " val loss  0.7075609901533698\n",
      " train accuracy  0.5330416762606494\n",
      " val accuracy  0.4894009216589862\n",
      "------Early stopping after epoch: 5 ---------\n",
      " train loss  0.6529455288143277\n",
      " val loss  0.7144673571608583\n",
      " train accuracy  0.6097167856320516\n",
      " val accuracy  0.5207373271889401\n",
      "Processing fold:  3\n",
      "---------epoch: 0 ---------\n",
      " train loss  0.6853642333028724\n",
      " val loss  0.6988290813112039\n",
      " train accuracy  0.5008058945429427\n",
      " val accuracy  0.4967741935483871\n",
      "------Early stopping after epoch: 6 ---------\n",
      " train loss  0.6473304068688119\n",
      " val loss  0.6960841306343606\n",
      " train accuracy  0.623301865070228\n",
      " val accuracy  0.5778801843317972\n",
      "Processing fold:  4\n",
      "---------epoch: 0 ---------\n",
      " train loss  0.685589373345038\n",
      " val loss  0.6749450402325748\n",
      " train accuracy  0.5166935298180981\n",
      " val accuracy  0.5778801843317972\n",
      "------Early stopping after epoch: 9 ---------\n",
      " train loss  0.6390362589583813\n",
      " val loss  0.6785969114523329\n",
      " train accuracy  0.6051116739580935\n",
      " val accuracy  0.6101382488479262\n",
      "Processing fold:  5\n",
      "---------epoch: 0 ---------\n",
      " train loss  0.6868746109939846\n",
      " val loss  0.6737962292086693\n",
      " train accuracy  0.48491825926778725\n",
      " val accuracy  0.5797235023041475\n",
      "---------epoch: 10 ---------\n",
      " train loss  0.648947030422519\n",
      " val loss  0.6638865949920795\n",
      " train accuracy  0.5843886714252821\n",
      " val accuracy  0.5815668202764976\n",
      "------Early stopping after epoch: 11 ---------\n",
      " train loss  0.643310546875\n",
      " val loss  0.6675343596990208\n",
      " train accuracy  0.5779415150817407\n",
      " val accuracy  0.5981566820276498\n",
      "cv_score:  0.5657142857142857\n",
      "current setting is  lr0 0.001 layer_num 1 lstm_dim 10 hidden_dim 20 p_drop 0.1\n",
      "current score is 0.5657142857142857\n",
      "best score is 0.5657142857142857\n",
      "best_parameters are {'lr0': 0.001, 'layer_num': 1, 'lstm_dim': 10, 'hidden_dim': 20, 'p_drop': 0.1}\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "k = 5\n",
    "\n",
    "params = {}\n",
    "params['lr0'] = [0.001]\n",
    "\n",
    "params['lstm_dim'] = [10]\n",
    "params['layer_num'] = [1]\n",
    "params['hidden_dim'] = [20]\n",
    "params['p_drop'] = [0.1]\n",
    "\n",
    "result = []\n",
    "\n",
    "best_params = {}\n",
    "best_score = 0\n",
    "for lr0 in params['lr0']:\n",
    "    for layer_num in params['layer_num']:\n",
    "        for lstm_dim in params['lstm_dim']:\n",
    "            for hidden_dim in params['hidden_dim']:\n",
    "                for p_drop in params['p_drop']:\n",
    "                    inputs = rnn_bidirect, init_word_embs, weights, lstm_dim, layer_num, hidden_dim,p_drop\n",
    "                    score = LSTM_kFold(k, epochs,lr0,*inputs)\n",
    "                    result.append(score)\n",
    "\n",
    "                    print('current setting is ','lr0',lr0,'layer_num',layer_num,'lstm_dim',lstm_dim,'hidden_dim',hidden_dim,'p_drop',p_drop)\n",
    "                    print('current score is',score)\n",
    "\n",
    "                    if score>best_score:\n",
    "                        best_score = score\n",
    "                        best_params['lr0'] = lr0\n",
    "                        best_params['layer_num'] = layer_num\n",
    "                        best_params['lstm_dim'] = lstm_dim\n",
    "                        best_params['hidden_dim'] = hidden_dim\n",
    "                        best_params['p_drop'] = p_drop\n",
    "\n",
    "print('best score is', best_score)\n",
    "print('best_parameters are', best_params)\n",
    "                    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(train_loss,label = \"train_loss\")\n",
    "# plt.plot(val_loss,label = \"val_loss\")\n",
    "# plt.xlabel('epoch')\n",
    "# plt.ylabel('loss')\n",
    "# plt.ylim(bottom = 0)\n",
    "# plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(train_acc,label = \"train_acc\")\n",
    "# plt.plot(dev_acc,label = \"val_acc\")\n",
    "# plt.xlabel('epoch')\n",
    "# plt.ylabel('accuracy')\n",
    "# plt.ylim(bottom = 0,top = 1)\n",
    "\n",
    "# plt.legend()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1-layer hidden attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_attn(torch.nn.Module):\n",
    "    def __init__(self, rnn_bidirect, glove, load_weights, lstm_dim=10, layer_num=1, hidden_dim = 20,p_drop=0.1):\n",
    "\n",
    "        # TODO: Declare LSTM model architecture\n",
    "        super(LSTM_attn, self).__init__()\n",
    "\n",
    "        if glove == 'glove':\n",
    "            self.embedding = nn.Embedding.from_pretrained(load_weights, freeze=True)\n",
    "        else:\n",
    "            self.embedding = nn.Embedding.from_pretrained(load_weights, freeze=False)\n",
    "\n",
    "        if rnn_bidirect:\n",
    "            self.lstm = nn.LSTM(50, lstm_dim, num_layers=layer_num, bias=False, bidirectional = True) # one layer, bidirectional\n",
    "            self.hidden_layer = nn.Linear(4*lstm_dim,hidden_dim)\n",
    "            \n",
    "            self.softmax = nn.Softmax(dim=1)\n",
    "            self.ws = nn.Linear(2*lstm_dim, 2*lstm_dim, bias=False)\n",
    "\n",
    "        else:\n",
    "            self.lstm = nn.LSTM(50, lstm_dim, num_layers=layer_num, bias=False) # two layers\n",
    "            self.hidden_layer = nn.Linear(2*lstm_dim,hidden_dim)\n",
    "\n",
    "            self.softmax = nn.Softmax(dim=1)\n",
    "            self.ws = nn.Linear(lstm_dim, lstm_dim, bias=False)\n",
    "\n",
    "        # final hidden layer\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=p_drop)\n",
    "        self.output_layer = nn.Linear(hidden_dim, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, s1, s2, idx1, idx2):\n",
    "        # TODO: Implement LSTM forward pass\n",
    "        embed1 = self.embedding(s1).squeeze(0)\n",
    "\t\t# print(\"embedding_squeeze\",self.embedding(s1).squeeze(0).shape)\n",
    "        lstm_out1,(_,_) = self.lstm(embed1)\n",
    "        # print('lstm',lstm_out1.shape)\n",
    "\n",
    "        ws1 = self.ws(lstm_out1)\n",
    "        # print('ws1',ws1.shape)\n",
    "\n",
    "        similar1 = torch.bmm(ws1.unsqueeze(0), torch.transpose(lstm_out1,0,1).unsqueeze(0))\n",
    "        # print('similar1',similar1.shape)\n",
    "        \n",
    "        attn1 = self.softmax(torch.squeeze(similar1))\n",
    "        # print('attn1',attn1.shape)\n",
    "\n",
    "        context1 = torch.squeeze(torch.bmm(attn1.unsqueeze(0), lstm_out1.unsqueeze(0)))\n",
    "        # print('context1',context1.shape)\n",
    "\n",
    "        # condense1 = self.condense(context1)\n",
    "        # print('condense1',condense1.shape)\n",
    "\n",
    "        embed2 = self.embedding(s2).squeeze(0)\n",
    "        lstm_out2,(_,_) = self.lstm(embed2)\n",
    "        \n",
    "        ws2 = self.ws(lstm_out2)\n",
    "        similar2 = torch.bmm(ws2.unsqueeze(0), torch.transpose(lstm_out2,0,1).unsqueeze(0))    \n",
    "        attn2 = self.softmax(torch.squeeze(similar2))\n",
    "        context2 = torch.squeeze(torch.bmm(attn2.unsqueeze(0), lstm_out2.unsqueeze(0)))\n",
    "\n",
    "        # cat_rep = torch.cat((lstm_out1[-1,:].unsqueeze(0), lstm_out2[-1,:].unsqueeze(0)),1)\n",
    "\n",
    "        cat_rep = torch.cat((context1[-1,:].unsqueeze(0), context2[-1,:].unsqueeze(0)),1)\n",
    "        ratio1, ratio2 = attn1[-1,:], attn2[-1,:]\n",
    "\n",
    "        hidden_rep = self.hidden_layer(cat_rep)\n",
    "        relu_rep = self.relu(hidden_rep)\n",
    "\t\t# print(\"hidden\",hidden_rep.shape)\n",
    "        drop = self.dropout(relu_rep)\n",
    "\t\t# print(\"drop\",drop.shape)\n",
    "        output = self.output_layer(drop)\n",
    "        # print(\"ouput\",output.shape)\n",
    "\n",
    "        output = self.sigmoid(output)\n",
    "        # print(\"sigmoid\",output.shape)\n",
    "\n",
    "        return output.squeeze(0).squeeze(0),ratio1, ratio2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Training and validation loop here\n",
    "\n",
    "lr0 = 0.001\n",
    "epochs = 20\n",
    "patience = 5\n",
    "\n",
    "def training_attn(model,train_dataset, valid_dataset,lr0,epochs):\n",
    "    ce = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr0)\n",
    "\n",
    "    train_acc = []\n",
    "    dev_acc = []\n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "    # test_output = []\n",
    "\n",
    "    best_val_loss = 1\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        #print(\"Epoch:\",i)\n",
    "        total_loss = 0\n",
    "\n",
    "        for i in range(len(train_dataset)):\n",
    "            sample = train_dataset[i]\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # a) calculate probs / get an output\n",
    "            if init_word_embs == \"glove\":\n",
    "                s1 = sen2glove(sample[\"sentence1\"],glove_embs)\n",
    "                s2 = sen2glove(sample[\"sentence2\"],glove_embs)\n",
    "\n",
    "                # print(sample[\"sentence1\"])\n",
    "                # print(sample[\"sentence2\"])\n",
    "\n",
    "            else:\n",
    "                s1 = sen2vec(sample[\"sentence1\"])\n",
    "                s2 = sen2vec(sample[\"sentence2\"])\n",
    "\n",
    "            # print(int(sample['idx1']))\n",
    "            # print(int(sample['idx2']))\n",
    "\n",
    "            y_raw,_, _ = model(s1,s2,int(sample['idx1']),int(sample['idx2']))\n",
    "            \n",
    "\n",
    "            y = tensor(float(sample[\"label\"]))\n",
    "            \n",
    "            # b) compute loss\n",
    "            loss = ce(y_raw,y)\n",
    "            total_loss += loss\n",
    "\n",
    "            # c) get the gradient\n",
    "            loss.backward()\n",
    "\n",
    "            # d) update the weights\n",
    "            optimizer.step()\n",
    "        train_loss.append(total_loss.item()/len(train_dataset))\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        score = 0\n",
    "        \n",
    "        for i in range(len(train_dataset)):\n",
    "            sample = train_dataset[i]\n",
    "\n",
    "            # a) calculate probs / get an output\n",
    "            if init_word_embs == \"glove\":\n",
    "                s1 = sen2glove(sample[\"sentence1\"],glove_embs)\n",
    "                s2 = sen2glove(sample[\"sentence2\"],glove_embs)\n",
    "            else:\n",
    "                s1 = sen2vec(sample[\"sentence1\"])\n",
    "                s2 = sen2vec(sample[\"sentence2\"])\n",
    "\n",
    "            y_raw,_, _ = model(s1,s2,int(sample['idx1']),int(sample['idx2']))\n",
    "\n",
    "            result = True if y_raw >= 0.5 else False\n",
    "\n",
    "            if bool(result) == sample[\"label\"]:\n",
    "                score += 1\n",
    "        \n",
    "        train_acc.append(score/len(train_dataset))\n",
    "        \n",
    "\n",
    "        score = 0\n",
    "        total_loss = 0\n",
    "        ratios1 = []\n",
    "        ratios2 = []\n",
    "        for i in range(len(valid_dataset)):\n",
    "            sample = valid_dataset[i]\n",
    "            # a) calculate probs / get an output\n",
    "            if init_word_embs == \"glove\":\n",
    "                s1 = sen2glove(sample[\"sentence1\"],glove_embs)\n",
    "                s2 = sen2glove(sample[\"sentence2\"],glove_embs)\n",
    "            else:\n",
    "                s1 = sen2vec(sample[\"sentence1\"])\n",
    "                s2 = sen2vec(sample[\"sentence2\"])\n",
    "\n",
    "            y_raw,ratio1, ratio2 = model(s1,s2,int(sample['idx1']),int(sample['idx2']))\n",
    "            \n",
    "            ratios1.append(ratio1)\n",
    "            ratios2.append(ratio2)\n",
    "            \n",
    "            y = tensor(float(sample[\"label\"]))\n",
    "            loss = ce(y_raw,y)\n",
    "            total_loss += loss\n",
    "\n",
    "            result = True if y_raw >= 0.5 else False\n",
    "            if bool(result) == sample[\"label\"]:\n",
    "                score += 1\n",
    "\n",
    "        val_loss.append(total_loss.item()/len(valid_dataset))\n",
    "        dev_acc.append(score/len(valid_dataset))\n",
    "\n",
    "\n",
    "\n",
    "        if epoch% 10 == 0:\n",
    "            print(\"---------epoch:\",epoch,\"---------\")\n",
    "            print(\" train loss \", train_loss[-1]) \n",
    "            print(\" val loss \", val_loss[-1])\n",
    "            print(\" train accuracy \",train_acc[-1])\n",
    "            print(\" val accuracy \",dev_acc[-1])\n",
    "\n",
    "        # check if validation loss has improvedval loss < best val loss:\n",
    "        if val_loss[-1] < best_val_loss:\n",
    "            best_val_loss = val_loss[-1] \n",
    "            early_stop_counter = 0\n",
    "        else:\n",
    "            early_stop_counter += 1\n",
    "            if early_stop_counter >= patience:\n",
    "                print(\"------Early stopping after epoch:\",epoch,\"---------\")\n",
    "                print(\" train loss \", train_loss[-1]) \n",
    "                print(\" val loss \", val_loss[-1]) \n",
    "                print(\" train accuracy \",train_acc[-1])\n",
    "                print(\" val accuracy \",dev_acc[-1])\n",
    "                return train_loss,val_loss, train_acc,dev_acc, ratios1, ratios2\n",
    "\n",
    "\n",
    "    print(\"---------endng epoch:\",epoch,\"---------\")\n",
    "    print(\" train loss \", train_loss[-1]) \n",
    "    print(\" val loss \", val_loss[-1]) \n",
    "    print(\" train accuracy \",train_acc[-1])\n",
    "    print(\" val accuracy \",dev_acc[-1])\n",
    "    return train_loss,val_loss, train_acc,dev_acc,ratios1, ratios2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------epoch: 0 ---------\n",
      " train loss  0.6818771193637159\n",
      " val loss  0.6949111316645034\n",
      " train accuracy  0.5523212969786293\n",
      " val accuracy  0.5172413793103449\n",
      "---------epoch: 10 ---------\n",
      " train loss  0.5850144613289886\n",
      " val loss  0.735172953351538\n",
      " train accuracy  0.6825718496683861\n",
      " val accuracy  0.5579937304075235\n",
      "------Early stopping after epoch: 10 ---------\n",
      " train loss  0.5850144613289886\n",
      " val loss  0.735172953351538\n",
      " train accuracy  0.6825718496683861\n",
      " val accuracy  0.5579937304075235\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5579937304075235"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr0 = 0.001\n",
    "epochs = 20\n",
    "model = LSTM_attn(rnn_bidirect,init_word_embs, weights).to(torch_device)\n",
    "train_loss,val_loss,train_acc,dev_acc,ratios1, ratios2 = training_attn(model,train_data, dev_data,lr0,epochs)\n",
    "dev_acc[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'board'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_data[0]['word']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1769, 0.2491, 0.2834, 0.2905], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratios1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Room and board .'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_data[0]['sentence1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1628, 0.6319, 0.1563, 0.0234, 0.0101, 0.0098, 0.0056],\n",
       "       grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratios2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'He nailed boards across the windows .'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_data[0]['sentence2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTM_attn_kFold(k,epochs,lr0,*inputs):\n",
    "    \n",
    "    num_val_samples = len(train_data)//k\n",
    "    cv_score = []\n",
    "    for i in range(k):\n",
    "        print('Processing fold: ', i + 1)\n",
    "        \"\"\"%%%% Initiate new model %%%%\"\"\" #in every fold\n",
    "        model = LSTM_attn(*inputs).to(torch_device)\n",
    "\n",
    "        valid_idx = np.arange(len(train_data))[i * num_val_samples:(i + 1) * num_val_samples]\n",
    "        train_idx = np.concatenate([np.arange(len(train_data))[:i * num_val_samples], np.arange(len(train_data))[(i + 1) * num_val_samples:]], axis=0)\n",
    "        \n",
    "        train_dataset = Subset(train_data, train_idx)\n",
    "        valid_dataset = Subset(train_data, valid_idx)\n",
    "\n",
    "        \n",
    "        _,_,_,valid_acc, _,_ = training_attn(model,train_dataset, valid_dataset,lr0,epochs)\n",
    "        cv_score.append(valid_acc[-1])\n",
    "    \n",
    "    print('cv_score: ',sum(cv_score)/len(cv_score))\n",
    "\n",
    "    return sum(cv_score)/len(cv_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing fold:  1\n",
      "---------epoch: 0 ---------\n",
      " train loss  0.6853163945573336\n",
      " val loss  0.7199211999567973\n",
      " train accuracy  0.5535344232097629\n",
      " val accuracy  0.46912442396313364\n",
      "---------epoch: 10 ---------\n",
      " train loss  0.6048165465619963\n",
      " val loss  0.7207437963529666\n",
      " train accuracy  0.6797144830762146\n",
      " val accuracy  0.5465437788018433\n",
      "------Early stopping after epoch: 10 ---------\n",
      " train loss  0.6048165465619963\n",
      " val loss  0.7207437963529666\n",
      " train accuracy  0.6797144830762146\n",
      " val accuracy  0.5465437788018433\n",
      "Processing fold:  2\n",
      "---------epoch: 0 ---------\n",
      " train loss  0.6816294382663194\n",
      " val loss  0.7032380134828629\n",
      " train accuracy  0.5447847110292424\n",
      " val accuracy  0.5105990783410138\n",
      "------Early stopping after epoch: 9 ---------\n",
      " train loss  0.6014142617250461\n",
      " val loss  0.7172439786146313\n",
      " train accuracy  0.6509325351139765\n",
      " val accuracy  0.5548387096774193\n",
      "Processing fold:  3\n",
      "---------epoch: 0 ---------\n",
      " train loss  0.6804142441860465\n",
      " val loss  0.6957866615963422\n",
      " train accuracy  0.5376467879346074\n",
      " val accuracy  0.5235023041474655\n",
      "------Early stopping after epoch: 9 ---------\n",
      " train loss  0.600449560535632\n",
      " val loss  0.6916739046298963\n",
      " train accuracy  0.6705042597282984\n",
      " val accuracy  0.5824884792626728\n",
      "Processing fold:  4\n",
      "---------epoch: 0 ---------\n",
      " train loss  0.6847517174728011\n",
      " val loss  0.6721525552635369\n",
      " train accuracy  0.531429887174764\n",
      " val accuracy  0.5778801843317972\n",
      "------Early stopping after epoch: 9 ---------\n",
      " train loss  0.612120865293288\n",
      " val loss  0.6817034602714573\n",
      " train accuracy  0.6585309693760074\n",
      " val accuracy  0.5824884792626728\n",
      "Processing fold:  5\n",
      "---------epoch: 0 ---------\n",
      " train loss  0.6834063862645349\n",
      " val loss  0.6668976814516129\n",
      " train accuracy  0.545935988947732\n",
      " val accuracy  0.5806451612903226\n",
      "------Early stopping after epoch: 7 ---------\n",
      " train loss  0.6177334575754087\n",
      " val loss  0.6924449727282546\n",
      " train accuracy  0.6246833985724154\n",
      " val accuracy  0.5972350230414747\n",
      "cv_score:  0.5727188940092166\n",
      "current setting is  lr0 0.001 layer_num 1 lstm_dim 10 hidden_dim 20 p_drop 0.1\n",
      "current score is 0.5727188940092166\n",
      "best score is 0.5727188940092166\n",
      "best_parameters are {'lr0': 0.001, 'layer_num': 1, 'lstm_dim': 10, 'hidden_dim': 20, 'p_drop': 0.1}\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "k = 5\n",
    "\n",
    "params = {}\n",
    "params['lr0'] = [0.001]\n",
    "\n",
    "params['lstm_dim'] = [10]\n",
    "params['layer_num'] = [1]\n",
    "params['hidden_dim'] = [20]\n",
    "params['p_drop'] = [0.1]\n",
    "\n",
    "result = []\n",
    "\n",
    "best_params = {}\n",
    "best_score = 0\n",
    "for lr0 in params['lr0']:\n",
    "    for layer_num in params['layer_num']:\n",
    "        for lstm_dim in params['lstm_dim']:\n",
    "            for hidden_dim in params['hidden_dim']:\n",
    "                for p_drop in params['p_drop']:\n",
    "                    inputs = rnn_bidirect, init_word_embs, weights, lstm_dim, layer_num, hidden_dim,p_drop\n",
    "                    score = LSTM_attn_kFold(k, epochs,lr0,*inputs)\n",
    "                    result.append(score)\n",
    "\n",
    "                    print('current setting is ','lr0',lr0,'layer_num',layer_num,'lstm_dim',lstm_dim,'hidden_dim',hidden_dim,'p_drop',p_drop)\n",
    "                    print('current score is',score)\n",
    "\n",
    "                    if score>best_score:\n",
    "                        best_score = score\n",
    "                        best_params['lr0'] = lr0\n",
    "                        best_params['layer_num'] = layer_num\n",
    "                        best_params['lstm_dim'] = lstm_dim\n",
    "                        best_params['hidden_dim'] = hidden_dim\n",
    "                        best_params['p_drop'] = p_drop\n",
    "\n",
    "print('best score is', best_score)\n",
    "print('best_parameters are', best_params)\n",
    "                    \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1-layer target attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_attn_target(torch.nn.Module):\n",
    "    def __init__(self, rnn_bidirect, glove, load_weights, lstm_dim=10, layer_num=1, hidden_dim = 20,p_drop=0.1):\n",
    "\n",
    "        # TODO: Declare LSTM model architecture\n",
    "        super(LSTM_attn_target, self).__init__()\n",
    "\n",
    "        if glove == 'glove':\n",
    "            self.embedding = nn.Embedding.from_pretrained(load_weights, freeze=True)\n",
    "        else:\n",
    "            self.embedding = nn.Embedding.from_pretrained(load_weights, freeze=False)\n",
    "\n",
    "        if rnn_bidirect:\n",
    "            self.lstm = nn.LSTM(50, lstm_dim, num_layers=layer_num, bias=False, bidirectional = True) # one layer, bidirectional\n",
    "            self.hidden_layer = nn.Linear(4*lstm_dim,hidden_dim)\n",
    "            \n",
    "            self.softmax = nn.Softmax(dim=1)\n",
    "            self.ws = nn.Linear(2*lstm_dim, 2*lstm_dim, bias=False)\n",
    "\n",
    "        else:\n",
    "            self.lstm = nn.LSTM(50, lstm_dim, num_layers=layer_num, bias=False) # two layers\n",
    "            self.hidden_layer = nn.Linear(2*lstm_dim,hidden_dim)\n",
    "\n",
    "            self.softmax = nn.Softmax(dim=1)\n",
    "            self.ws = nn.Linear(lstm_dim, lstm_dim, bias=False)\n",
    "\n",
    "        # final hidden layer\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=p_drop)\n",
    "        self.output_layer = nn.Linear(hidden_dim, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, s1, s2, idx1, idx2):\n",
    "        # TODO: Implement LSTM forward pass\n",
    "        embed1 = self.embedding(s1).squeeze(0)\n",
    "\t\t# print(\"embedding_squeeze\",self.embedding(s1).squeeze(0).shape)\n",
    "        lstm_out1,(_,_) = self.lstm(embed1)\n",
    "        # print('lstm',lstm_out1.shape)\n",
    "\n",
    "        ws1 = self.ws(lstm_out1)\n",
    "        # print('ws1',ws1.shape)\n",
    "\n",
    "        similar1 = torch.bmm(ws1.unsqueeze(0), torch.transpose(lstm_out1,0,1).unsqueeze(0))\n",
    "        # print('similar1',similar1.shape)\n",
    "        \n",
    "        attn1 = self.softmax(torch.squeeze(similar1))\n",
    "        # print('attn1',attn1.shape)\n",
    "\n",
    "        context1 = torch.squeeze(torch.bmm(attn1.unsqueeze(0), lstm_out1.unsqueeze(0)))\n",
    "        # print('context1',context1.shape)\n",
    "\n",
    "        # condense1 = self.condense(context1)\n",
    "        # print('condense1',condense1.shape)\n",
    "\n",
    "        embed2 = self.embedding(s2).squeeze(0)\n",
    "        lstm_out2,(_,_) = self.lstm(embed2)\n",
    "        \n",
    "        ws2 = self.ws(lstm_out2)\n",
    "        similar2 = torch.bmm(ws2.unsqueeze(0), torch.transpose(lstm_out2,0,1).unsqueeze(0))    \n",
    "        attn2 = self.softmax(torch.squeeze(similar2))\n",
    "        context2 = torch.squeeze(torch.bmm(attn2.unsqueeze(0), lstm_out2.unsqueeze(0)))\n",
    "\n",
    "        # cat_rep = torch.cat((lstm_out1[-1,:].unsqueeze(0), lstm_out2[-1,:].unsqueeze(0)),1)\n",
    "        try:\n",
    "            cat_rep = torch.cat((context1[idx1,:].unsqueeze(0), context2[idx2,:].unsqueeze(0)),1)\n",
    "            ratio1, ratio2 = attn1[idx1,:], attn2[idx2,:]\n",
    "        except:\n",
    "            cat_rep = torch.cat((context1[-1,:].unsqueeze(0), context2[-1,:].unsqueeze(0)),1)\n",
    "            ratio1, ratio2 = attn1[-1,:], attn2[-1,:]\n",
    "            # in total of 8 wrong indexing samples\n",
    "            # print('wrong index')\n",
    "\n",
    "\n",
    "\n",
    "        hidden_rep = self.hidden_layer(cat_rep)\n",
    "        relu_rep = self.relu(hidden_rep)\n",
    "\t\t# print(\"hidden\",hidden_rep.shape)\n",
    "        drop = self.dropout(relu_rep)\n",
    "\t\t# print(\"drop\",drop.shape)\n",
    "        output = self.output_layer(drop)\n",
    "        # print(\"ouput\",output.shape)\n",
    "\n",
    "        output = self.sigmoid(output)\n",
    "        # print(\"sigmoid\",output.shape)\n",
    "\n",
    "        return output.squeeze(0).squeeze(0),ratio1, ratio2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Training and validation loop here\n",
    "\n",
    "lr0 = 0.001\n",
    "epochs = 20\n",
    "patience = 5\n",
    "\n",
    "def training_attn(model,train_dataset, valid_dataset,lr0,epochs):\n",
    "    ce = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr0)\n",
    "\n",
    "    train_acc = []\n",
    "    dev_acc = []\n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "    # test_output = []\n",
    "\n",
    "    best_val_loss = 1\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        #print(\"Epoch:\",i)\n",
    "        total_loss = 0\n",
    "\n",
    "        for i in range(len(train_dataset)):\n",
    "            sample = train_dataset[i]\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # a) calculate probs / get an output\n",
    "            if init_word_embs == \"glove\":\n",
    "                s1 = sen2glove(sample[\"sentence1\"],glove_embs)\n",
    "                s2 = sen2glove(sample[\"sentence2\"],glove_embs)\n",
    "\n",
    "                # print(sample[\"sentence1\"])\n",
    "                # print(sample[\"sentence2\"])\n",
    "\n",
    "            else:\n",
    "                s1 = sen2vec(sample[\"sentence1\"])\n",
    "                s2 = sen2vec(sample[\"sentence2\"])\n",
    "\n",
    "            # print(int(sample['idx1']))\n",
    "            # print(int(sample['idx2']))\n",
    "\n",
    "            y_raw,_, _ = model(s1,s2,int(sample['idx1']),int(sample['idx2']))\n",
    "            \n",
    "\n",
    "            y = tensor(float(sample[\"label\"]))\n",
    "            \n",
    "            # b) compute loss\n",
    "            loss = ce(y_raw,y)\n",
    "            total_loss += loss\n",
    "\n",
    "            # c) get the gradient\n",
    "            loss.backward()\n",
    "\n",
    "            # d) update the weights\n",
    "            optimizer.step()\n",
    "        train_loss.append(total_loss.item()/len(train_dataset))\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        score = 0\n",
    "        \n",
    "        for i in range(len(train_dataset)):\n",
    "            sample = train_dataset[i]\n",
    "\n",
    "            # a) calculate probs / get an output\n",
    "            if init_word_embs == \"glove\":\n",
    "                s1 = sen2glove(sample[\"sentence1\"],glove_embs)\n",
    "                s2 = sen2glove(sample[\"sentence2\"],glove_embs)\n",
    "            else:\n",
    "                s1 = sen2vec(sample[\"sentence1\"])\n",
    "                s2 = sen2vec(sample[\"sentence2\"])\n",
    "\n",
    "            y_raw,_, _ = model(s1,s2,int(sample['idx1']),int(sample['idx2']))\n",
    "\n",
    "            result = True if y_raw >= 0.5 else False\n",
    "\n",
    "            if bool(result) == sample[\"label\"]:\n",
    "                score += 1\n",
    "        \n",
    "        train_acc.append(score/len(train_dataset))\n",
    "        \n",
    "\n",
    "        score = 0\n",
    "        total_loss = 0\n",
    "        ratios1 = []\n",
    "        ratios2 = []\n",
    "        for i in range(len(valid_dataset)):\n",
    "            sample = valid_dataset[i]\n",
    "            # a) calculate probs / get an output\n",
    "            if init_word_embs == \"glove\":\n",
    "                s1 = sen2glove(sample[\"sentence1\"],glove_embs)\n",
    "                s2 = sen2glove(sample[\"sentence2\"],glove_embs)\n",
    "            else:\n",
    "                s1 = sen2vec(sample[\"sentence1\"])\n",
    "                s2 = sen2vec(sample[\"sentence2\"])\n",
    "\n",
    "            y_raw,ratio1, ratio2 = model(s1,s2,int(sample['idx1']),int(sample['idx2']))\n",
    "            \n",
    "            ratios1.append(ratio1)\n",
    "            ratios2.append(ratio2)\n",
    "            \n",
    "            y = tensor(float(sample[\"label\"]))\n",
    "            loss = ce(y_raw,y)\n",
    "            total_loss += loss\n",
    "\n",
    "            result = True if y_raw >= 0.5 else False\n",
    "            if bool(result) == sample[\"label\"]:\n",
    "                score += 1\n",
    "\n",
    "        val_loss.append(total_loss.item()/len(valid_dataset))\n",
    "        dev_acc.append(score/len(valid_dataset))\n",
    "\n",
    "\n",
    "\n",
    "        if epoch% 10 == 0:\n",
    "            print(\"---------epoch:\",epoch,\"---------\")\n",
    "            print(\" train loss \", train_loss[-1]) \n",
    "            print(\" val loss \", val_loss[-1])\n",
    "            print(\" train accuracy \",train_acc[-1])\n",
    "            print(\" val accuracy \",dev_acc[-1])\n",
    "\n",
    "        # check if validation loss has improvedval loss < best val loss:\n",
    "        if val_loss[-1] < best_val_loss:\n",
    "            best_val_loss = val_loss[-1] \n",
    "            early_stop_counter = 0\n",
    "        else:\n",
    "            early_stop_counter += 1\n",
    "            if early_stop_counter >= patience:\n",
    "                print(\"------Early stopping after epoch:\",epoch,\"---------\")\n",
    "                print(\" train loss \", train_loss[-1]) \n",
    "                print(\" val loss \", val_loss[-1]) \n",
    "                print(\" train accuracy \",train_acc[-1])\n",
    "                print(\" val accuracy \",dev_acc[-1])\n",
    "                return train_loss,val_loss, train_acc,dev_acc, ratios1, ratios2\n",
    "\n",
    "\n",
    "    print(\"---------endng epoch:\",epoch,\"---------\")\n",
    "    print(\" train loss \", train_loss[-1]) \n",
    "    print(\" val loss \", val_loss[-1]) \n",
    "    print(\" train accuracy \",train_acc[-1])\n",
    "    print(\" val accuracy \",dev_acc[-1])\n",
    "    return train_loss,val_loss, train_acc,dev_acc,ratios1, ratios2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------epoch: 0 ---------\n",
      " train loss  0.6828579097964259\n",
      " val loss  0.6974765215547855\n",
      " train accuracy  0.5418201915991157\n",
      " val accuracy  0.5156739811912225\n",
      "------Early stopping after epoch: 6 ---------\n",
      " train loss  0.6168415741439066\n",
      " val loss  0.7245336072198276\n",
      " train accuracy  0.6573323507737656\n",
      " val accuracy  0.5470219435736677\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5470219435736677"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr0 = 0.001\n",
    "epochs = 20\n",
    "model = LSTM_attn_target(rnn_bidirect,init_word_embs, weights).to(torch_device)\n",
    "train_loss,val_loss,train_acc,dev_acc,ratios1, ratios2 = training_attn(model,train_data, dev_data,lr0,epochs)\n",
    "dev_acc[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_idx = []\n",
    "wrong_ratio1 = []\n",
    "wrong_ratio2 = []\n",
    "\n",
    "for i in range(len(train_data)):\n",
    "    sample = train_data[i]\n",
    "\n",
    "    # a) calculate probs / get an output\n",
    "    if init_word_embs == \"glove\":\n",
    "        s1 = sen2glove(sample[\"sentence1\"],glove_embs)\n",
    "        s2 = sen2glove(sample[\"sentence2\"],glove_embs)\n",
    "    else:\n",
    "        s1 = sen2vec(sample[\"sentence1\"])\n",
    "        s2 = sen2vec(sample[\"sentence2\"])\n",
    "\n",
    "    \n",
    "    y_raw,ratio1, ratio2 = model(s1,s2,int(sample['idx1']),int(sample['idx2']))\n",
    "\n",
    "\n",
    "\n",
    "    result = True if y_raw >= 0.5 else False\n",
    "\n",
    "    if bool(result) != sample[\"label\"]:\n",
    "        wrong_idx.append(i)\n",
    "        wrong_ratio1.append(ratio1)\n",
    "        wrong_ratio2.append(ratio2)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "False\n",
      "Do you think the sofa will go through the door ?\n",
      "tensor([0.2125, 0.1916, 0.1303, 0.1023, 0.1120, 0.1073, 0.1439],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "go\n",
      "Messages must go through diplomatic channels .\n",
      "tensor([0.0855, 0.2059, 0.1595, 0.0445, 0.3542, 0.0501, 0.0507, 0.0046, 0.0025,\n",
      "        0.0010, 0.0415], grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "print(i)\n",
    "sample = train_data[wrong_idx[i]]\n",
    "print(sample[\"label\"])\n",
    "print(sample[\"sentence2\"])\n",
    "print(wrong_ratio1[i])\n",
    "print(sample['word'])\n",
    "print(sample['sentence1'])\n",
    "print(wrong_ratio2[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'board'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_data[0]['word']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2276, 0.2472, 0.2793, 0.2459], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratios1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Room and board .'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_data[0]['sentence1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0351, 0.3365, 0.1489, 0.1453, 0.1200, 0.1672, 0.0469],\n",
       "       grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratios2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'He nailed boards across the windows .'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_data[0]['sentence2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTM_attn_kFold(k,epochs,lr0,*inputs):\n",
    "    \n",
    "    num_val_samples = len(train_data)//k\n",
    "    cv_score = []\n",
    "    for i in range(k):\n",
    "        print('Processing fold: ', i + 1)\n",
    "        \"\"\"%%%% Initiate new model %%%%\"\"\" #in every fold\n",
    "        model = LSTM_attn_target(*inputs).to(torch_device)\n",
    "\n",
    "        valid_idx = np.arange(len(train_data))[i * num_val_samples:(i + 1) * num_val_samples]\n",
    "        train_idx = np.concatenate([np.arange(len(train_data))[:i * num_val_samples], np.arange(len(train_data))[(i + 1) * num_val_samples:]], axis=0)\n",
    "        \n",
    "        train_dataset = Subset(train_data, train_idx)\n",
    "        valid_dataset = Subset(train_data, valid_idx)\n",
    "\n",
    "        \n",
    "        _,_,_,valid_acc, _,_ = training_attn(model,train_dataset, valid_dataset,lr0,epochs)\n",
    "        cv_score.append(valid_acc[-1])\n",
    "    \n",
    "    print('cv_score: ',sum(cv_score)/len(cv_score))\n",
    "\n",
    "    return sum(cv_score)/len(cv_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing fold:  1\n",
      "---------epoch: 0 ---------\n",
      " train loss  0.6856035394600507\n",
      " val loss  0.7217603551627304\n",
      " train accuracy  0.5493898227032006\n",
      " val accuracy  0.44608294930875575\n",
      "------Early stopping after epoch: 9 ---------\n",
      " train loss  0.6168800053606378\n",
      " val loss  0.7266809700820852\n",
      " train accuracy  0.6437946120193415\n",
      " val accuracy  0.5124423963133641\n",
      "Processing fold:  2\n",
      "---------epoch: 0 ---------\n",
      " train loss  0.6807003210086634\n",
      " val loss  0.7029710901497696\n",
      " train accuracy  0.5415611328574718\n",
      " val accuracy  0.4967741935483871\n",
      "---------epoch: 10 ---------\n",
      " train loss  0.577672752399695\n",
      " val loss  0.7263838948192685\n",
      " train accuracy  0.6792539719088188\n",
      " val accuracy  0.5714285714285714\n",
      "------Early stopping after epoch: 10 ---------\n",
      " train loss  0.577672752399695\n",
      " val loss  0.7263838948192685\n",
      " train accuracy  0.6792539719088188\n",
      " val accuracy  0.5714285714285714\n",
      "Processing fold:  3\n",
      "---------epoch: 0 ---------\n",
      " train loss  0.6852901422727665\n",
      " val loss  0.6963813184043779\n",
      " train accuracy  0.5162330186507023\n",
      " val accuracy  0.5013824884792627\n",
      "------Early stopping after epoch: 9 ---------\n",
      " train loss  0.5927005269794785\n",
      " val loss  0.7195951540898617\n",
      " train accuracy  0.6525443241998619\n",
      " val accuracy  0.5898617511520737\n",
      "Processing fold:  4\n",
      "---------epoch: 0 ---------\n",
      " train loss  0.683045712479133\n",
      " val loss  0.6718548612111175\n",
      " train accuracy  0.5270550310845038\n",
      " val accuracy  0.584331797235023\n",
      "------Early stopping after epoch: 9 ---------\n",
      " train loss  0.5808813212353212\n",
      " val loss  0.69359862156178\n",
      " train accuracy  0.685931383836058\n",
      " val accuracy  0.6018433179723502\n",
      "Processing fold:  5\n",
      "---------epoch: 0 ---------\n",
      " train loss  0.6866152923886139\n",
      " val loss  0.6700133996075749\n",
      " train accuracy  0.4941284826157034\n",
      " val accuracy  0.5898617511520737\n",
      "---------epoch: 10 ---------\n",
      " train loss  0.573240219984026\n",
      " val loss  0.6692493632092454\n",
      " train accuracy  0.6684319594750173\n",
      " val accuracy  0.6211981566820276\n",
      "------Early stopping after epoch: 10 ---------\n",
      " train loss  0.573240219984026\n",
      " val loss  0.6692493632092454\n",
      " train accuracy  0.6684319594750173\n",
      " val accuracy  0.6211981566820276\n",
      "cv_score:  0.5793548387096774\n",
      "current setting is  lr0 0.001 layer_num 1 lstm_dim 10 hidden_dim 20 p_drop 0.1\n",
      "current score is 0.5793548387096774\n",
      "best score is 0.5793548387096774\n",
      "best_parameters are {'lr0': 0.001, 'layer_num': 1, 'lstm_dim': 10, 'hidden_dim': 20, 'p_drop': 0.1}\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "k = 5\n",
    "\n",
    "params = {}\n",
    "params['lr0'] = [0.001]\n",
    "\n",
    "params['lstm_dim'] = [10]\n",
    "params['layer_num'] = [1]\n",
    "params['hidden_dim'] = [20]\n",
    "params['p_drop'] = [0.1]\n",
    "\n",
    "result = []\n",
    "\n",
    "best_params = {}\n",
    "best_score = 0\n",
    "for lr0 in params['lr0']:\n",
    "    for layer_num in params['layer_num']:\n",
    "        for lstm_dim in params['lstm_dim']:\n",
    "            for hidden_dim in params['hidden_dim']:\n",
    "                for p_drop in params['p_drop']:\n",
    "                    inputs = rnn_bidirect, init_word_embs, weights, lstm_dim, layer_num, hidden_dim,p_drop\n",
    "                    score = LSTM_attn_kFold(k, epochs,lr0,*inputs)\n",
    "                    result.append(score)\n",
    "\n",
    "                    print('current setting is ','lr0',lr0,'layer_num',layer_num,'lstm_dim',lstm_dim,'hidden_dim',hidden_dim,'p_drop',p_drop)\n",
    "                    print('current score is',score)\n",
    "\n",
    "                    if score>best_score:\n",
    "                        best_score = score\n",
    "                        best_params['lr0'] = lr0\n",
    "                        best_params['layer_num'] = layer_num\n",
    "                        best_params['lstm_dim'] = lstm_dim\n",
    "                        best_params['hidden_dim'] = hidden_dim\n",
    "                        best_params['p_drop'] = p_drop\n",
    "\n",
    "print('best score is', best_score)\n",
    "print('best_parameters are', best_params)\n",
    "                    \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1-layer attention + mean states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_attn_mean(torch.nn.Module):\n",
    "    def __init__(self, rnn_bidirect, glove, load_weights, lstm_dim=10, layer_num=1, hidden_dim = 20,p_drop=0.1):\n",
    "\n",
    "        # TODO: Declare LSTM model architecture\n",
    "        super(LSTM_attn_mean, self).__init__()\n",
    "\n",
    "        if glove == 'glove':\n",
    "            self.embedding = nn.Embedding.from_pretrained(load_weights, freeze=True)\n",
    "        else:\n",
    "            self.embedding = nn.Embedding.from_pretrained(load_weights, freeze=False)\n",
    "\n",
    "        if rnn_bidirect:\n",
    "            self.lstm = nn.LSTM(50, lstm_dim, num_layers=layer_num, bias=False, bidirectional = True) # one layer, bidirectional\n",
    "            self.hidden_layer = nn.Linear(4*lstm_dim,hidden_dim)\n",
    "            \n",
    "            self.softmax = nn.Softmax(dim=1)\n",
    "            self.ws = nn.Linear(2*lstm_dim, 2*lstm_dim, bias=False)\n",
    "\n",
    "        else:\n",
    "            self.lstm = nn.LSTM(50, lstm_dim, num_layers=layer_num, bias=False) # two layers\n",
    "            self.hidden_layer = nn.Linear(2*lstm_dim,hidden_dim)\n",
    "\n",
    "            self.softmax = nn.Softmax(dim=1)\n",
    "            self.ws = nn.Linear(lstm_dim, lstm_dim, bias=False)\n",
    "\n",
    "        # final hidden layer\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=p_drop)\n",
    "        self.output_layer = nn.Linear(hidden_dim, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, s1, s2, idx1, idx2):\n",
    "        # TODO: Implement LSTM forward pass\n",
    "        embed1 = self.embedding(s1).squeeze(0)\n",
    "\t\t# print(\"embedding_squeeze\",self.embedding(s1).squeeze(0).shape)\n",
    "        lstm_out1,(_,_) = self.lstm(embed1)\n",
    "        # print('lstm',lstm_out1.shape)\n",
    "\n",
    "        ws1 = self.ws(lstm_out1)\n",
    "        # print('ws1',ws1.shape)\n",
    "\n",
    "        similar1 = torch.bmm(ws1.unsqueeze(0), torch.transpose(lstm_out1,0,1).unsqueeze(0))\n",
    "        # print('similar1',similar1.shape)\n",
    "        \n",
    "        attn1 = self.softmax(torch.squeeze(similar1))\n",
    "        # print('attn1',attn1.shape)\n",
    "\n",
    "        context1 = torch.squeeze(torch.bmm(attn1.unsqueeze(0), lstm_out1.unsqueeze(0)))\n",
    "        # print('context1',context1.shape)\n",
    "\n",
    "        # condense1 = self.condense(context1)\n",
    "        # print('condense1',condense1.shape)\n",
    "\n",
    "        embed2 = self.embedding(s2).squeeze(0)\n",
    "        lstm_out2,(_,_) = self.lstm(embed2)\n",
    "        \n",
    "        ws2 = self.ws(lstm_out2)\n",
    "        similar2 = torch.bmm(ws2.unsqueeze(0), torch.transpose(lstm_out2,0,1).unsqueeze(0))    \n",
    "        attn2 = self.softmax(torch.squeeze(similar2))\n",
    "        context2 = torch.squeeze(torch.bmm(attn2.unsqueeze(0), lstm_out2.unsqueeze(0)))\n",
    "\n",
    "        # cat_rep = torch.cat((lstm_out1[-1,:].unsqueeze(0), lstm_out2[-1,:].unsqueeze(0)),1)\n",
    "        cat_rep = torch.cat((context1.mean(dim = 0).unsqueeze(0), context2.mean(dim = 0).unsqueeze(0)),1)\n",
    "        ratio1, ratio2 = None,None\n",
    "\n",
    "        hidden_rep = self.hidden_layer(cat_rep)\n",
    "        relu_rep = self.relu(hidden_rep)\n",
    "\t\t# print(\"hidden\",hidden_rep.shape)\n",
    "        drop = self.dropout(relu_rep)\n",
    "\t\t# print(\"drop\",drop.shape)\n",
    "        output = self.output_layer(drop)\n",
    "        # print(\"ouput\",output.shape)\n",
    "\n",
    "        output = self.sigmoid(output)\n",
    "        # print(\"sigmoid\",output.shape)\n",
    "\n",
    "        return output.squeeze(0).squeeze(0),ratio1, ratio2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Training and validation loop here\n",
    "\n",
    "lr0 = 0.001\n",
    "epochs = 20\n",
    "patience = 5\n",
    "\n",
    "def training_attn(model,train_dataset, valid_dataset,lr0,epochs):\n",
    "    ce = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr0)\n",
    "\n",
    "    train_acc = []\n",
    "    dev_acc = []\n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "    # test_output = []\n",
    "\n",
    "    best_val_loss = 1\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        #print(\"Epoch:\",i)\n",
    "        total_loss = 0\n",
    "\n",
    "        for i in range(len(train_dataset)):\n",
    "            sample = train_dataset[i]\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # a) calculate probs / get an output\n",
    "            if init_word_embs == \"glove\":\n",
    "                s1 = sen2glove(sample[\"sentence1\"],glove_embs)\n",
    "                s2 = sen2glove(sample[\"sentence2\"],glove_embs)\n",
    "\n",
    "                # print(sample[\"sentence1\"])\n",
    "                # print(sample[\"sentence2\"])\n",
    "\n",
    "            else:\n",
    "                s1 = sen2vec(sample[\"sentence1\"])\n",
    "                s2 = sen2vec(sample[\"sentence2\"])\n",
    "\n",
    "            # print(int(sample['idx1']))\n",
    "            # print(int(sample['idx2']))\n",
    "\n",
    "            y_raw,_, _ = model(s1,s2,int(sample['idx1']),int(sample['idx2']))\n",
    "            \n",
    "\n",
    "            y = tensor(float(sample[\"label\"]))\n",
    "            \n",
    "            # b) compute loss\n",
    "            loss = ce(y_raw,y)\n",
    "            total_loss += loss\n",
    "\n",
    "            # c) get the gradient\n",
    "            loss.backward()\n",
    "\n",
    "            # d) update the weights\n",
    "            optimizer.step()\n",
    "        train_loss.append(total_loss.item()/len(train_dataset))\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        score = 0\n",
    "        \n",
    "        for i in range(len(train_dataset)):\n",
    "            sample = train_dataset[i]\n",
    "\n",
    "            # a) calculate probs / get an output\n",
    "            if init_word_embs == \"glove\":\n",
    "                s1 = sen2glove(sample[\"sentence1\"],glove_embs)\n",
    "                s2 = sen2glove(sample[\"sentence2\"],glove_embs)\n",
    "            else:\n",
    "                s1 = sen2vec(sample[\"sentence1\"])\n",
    "                s2 = sen2vec(sample[\"sentence2\"])\n",
    "\n",
    "            y_raw,_, _ = model(s1,s2,int(sample['idx1']),int(sample['idx2']))\n",
    "\n",
    "            result = True if y_raw >= 0.5 else False\n",
    "\n",
    "            if bool(result) == sample[\"label\"]:\n",
    "                score += 1\n",
    "        \n",
    "        train_acc.append(score/len(train_dataset))\n",
    "        \n",
    "\n",
    "        score = 0\n",
    "        total_loss = 0\n",
    "        ratios1 = []\n",
    "        ratios2 = []\n",
    "        for i in range(len(valid_dataset)):\n",
    "            sample = valid_dataset[i]\n",
    "            # a) calculate probs / get an output\n",
    "            if init_word_embs == \"glove\":\n",
    "                s1 = sen2glove(sample[\"sentence1\"],glove_embs)\n",
    "                s2 = sen2glove(sample[\"sentence2\"],glove_embs)\n",
    "            else:\n",
    "                s1 = sen2vec(sample[\"sentence1\"])\n",
    "                s2 = sen2vec(sample[\"sentence2\"])\n",
    "\n",
    "            y_raw,ratio1, ratio2 = model(s1,s2,int(sample['idx1']),int(sample['idx2']))\n",
    "            \n",
    "            ratios1.append(ratio1)\n",
    "            ratios2.append(ratio2)\n",
    "            \n",
    "            y = tensor(float(sample[\"label\"]))\n",
    "            loss = ce(y_raw,y)\n",
    "            total_loss += loss\n",
    "\n",
    "            result = True if y_raw >= 0.5 else False\n",
    "            if bool(result) == sample[\"label\"]:\n",
    "                score += 1\n",
    "\n",
    "        val_loss.append(total_loss.item()/len(valid_dataset))\n",
    "        dev_acc.append(score/len(valid_dataset))\n",
    "\n",
    "\n",
    "\n",
    "        if epoch% 10 == 0:\n",
    "            print(\"---------epoch:\",epoch,\"---------\")\n",
    "            print(\" train loss \", train_loss[-1]) \n",
    "            print(\" val loss \", val_loss[-1])\n",
    "            print(\" train accuracy \",train_acc[-1])\n",
    "            print(\" val accuracy \",dev_acc[-1])\n",
    "\n",
    "        # check if validation loss has improvedval loss < best val loss:\n",
    "        if val_loss[-1] < best_val_loss:\n",
    "            best_val_loss = val_loss[-1] \n",
    "            early_stop_counter = 0\n",
    "        else:\n",
    "            early_stop_counter += 1\n",
    "            if early_stop_counter >= patience:\n",
    "                print(\"------Early stopping after epoch:\",epoch,\"---------\")\n",
    "                print(\" train loss \", train_loss[-1]) \n",
    "                print(\" val loss \", val_loss[-1]) \n",
    "                print(\" train accuracy \",train_acc[-1])\n",
    "                print(\" val accuracy \",dev_acc[-1])\n",
    "                return train_loss,val_loss, train_acc,dev_acc, ratios1, ratios2\n",
    "\n",
    "\n",
    "    print(\"---------endng epoch:\",epoch,\"---------\")\n",
    "    print(\" train loss \", train_loss[-1]) \n",
    "    print(\" val loss \", val_loss[-1]) \n",
    "    print(\" train accuracy \",train_acc[-1])\n",
    "    print(\" val accuracy \",dev_acc[-1])\n",
    "    return train_loss,val_loss, train_acc,dev_acc,ratios1, ratios2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------epoch: 0 ---------\n",
      " train loss  0.6811482507513127\n",
      " val loss  0.6969489214188627\n",
      " train accuracy  0.5467943994104643\n",
      " val accuracy  0.5219435736677116\n",
      "------Early stopping after epoch: 5 ---------\n",
      " train loss  0.6410744202514739\n",
      " val loss  0.6991034720011265\n",
      " train accuracy  0.6151436993367723\n",
      " val accuracy  0.5329153605015674\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5329153605015674"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr0 = 0.001\n",
    "epochs = 20\n",
    "model = LSTM_attn_mean(rnn_bidirect,init_word_embs, weights).to(torch_device)\n",
    "train_loss,val_loss,train_acc,dev_acc,ratios1, ratios2 = training_attn(model,train_data, dev_data,lr0,epochs)\n",
    "dev_acc[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'board'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_data[0]['word']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratios1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Room and board .'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_data[0]['sentence1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratios2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'He nailed boards across the windows .'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_data[0]['sentence2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTM_attn_kFold(k,epochs,lr0,*inputs):\n",
    "    \n",
    "    num_val_samples = len(train_data)//k\n",
    "    cv_score = []\n",
    "    for i in range(k):\n",
    "        print('Processing fold: ', i + 1)\n",
    "        \"\"\"%%%% Initiate new model %%%%\"\"\" #in every fold\n",
    "        model = LSTM_attn_mean(*inputs).to(torch_device)\n",
    "\n",
    "        valid_idx = np.arange(len(train_data))[i * num_val_samples:(i + 1) * num_val_samples]\n",
    "        train_idx = np.concatenate([np.arange(len(train_data))[:i * num_val_samples], np.arange(len(train_data))[(i + 1) * num_val_samples:]], axis=0)\n",
    "        \n",
    "        train_dataset = Subset(train_data, train_idx)\n",
    "        valid_dataset = Subset(train_data, valid_idx)\n",
    "\n",
    "        \n",
    "        _,_,_,valid_acc, _,_ = training_attn(model,train_dataset, valid_dataset,lr0,epochs)\n",
    "        cv_score.append(valid_acc[-1])\n",
    "    \n",
    "    print('cv_score: ',sum(cv_score)/len(cv_score))\n",
    "\n",
    "    return sum(cv_score)/len(cv_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing fold:  1\n",
      "---------epoch: 0 ---------\n",
      " train loss  0.6853550702999079\n",
      " val loss  0.7247401083669355\n",
      " train accuracy  0.5562974902141377\n",
      " val accuracy  0.4525345622119816\n",
      "------Early stopping after epoch: 9 ---------\n",
      " train loss  0.6035726829632454\n",
      " val loss  0.7373504216769873\n",
      " train accuracy  0.67142528206309\n",
      " val accuracy  0.5216589861751152\n",
      "Processing fold:  2\n",
      "---------epoch: 0 ---------\n",
      " train loss  0.6805212770550311\n",
      " val loss  0.7020435245355703\n",
      " train accuracy  0.5576790237163252\n",
      " val accuracy  0.5124423963133641\n",
      "---------epoch: 10 ---------\n",
      " train loss  0.5909926106845786\n",
      " val loss  0.7289483276929724\n",
      " train accuracy  0.67142528206309\n",
      " val accuracy  0.5483870967741935\n",
      "------Early stopping after epoch: 11 ---------\n",
      " train loss  0.5797322356917741\n",
      " val loss  0.7427091396349367\n",
      " train accuracy  0.6824775500805894\n",
      " val accuracy  0.5585253456221199\n",
      "Processing fold:  3\n",
      "---------epoch: 0 ---------\n",
      " train loss  0.6801883733522335\n",
      " val loss  0.6967186149913595\n",
      " train accuracy  0.5293575869214828\n",
      " val accuracy  0.5152073732718894\n",
      "---------epoch: 10 ---------\n",
      " train loss  0.6004999289445659\n",
      " val loss  0.7053605180731567\n",
      " train accuracy  0.6495510016117891\n",
      " val accuracy  0.5714285714285714\n",
      "------Early stopping after epoch: 11 ---------\n",
      " train loss  0.5909114365969664\n",
      " val loss  0.7209087881624424\n",
      " train accuracy  0.652083813032466\n",
      " val accuracy  0.5695852534562212\n",
      "Processing fold:  4\n",
      "---------epoch: 0 ---------\n",
      " train loss  0.6850376818659337\n",
      " val loss  0.6741686842957949\n",
      " train accuracy  0.516923785401796\n",
      " val accuracy  0.584331797235023\n",
      "---------epoch: 10 ---------\n",
      " train loss  0.5817958226600276\n",
      " val loss  0.7080880863875288\n",
      " train accuracy  0.6737278379000691\n",
      " val accuracy  0.5815668202764976\n",
      "------Early stopping after epoch: 10 ---------\n",
      " train loss  0.5817958226600276\n",
      " val loss  0.7080880863875288\n",
      " train accuracy  0.6737278379000691\n",
      " val accuracy  0.5815668202764976\n",
      "Processing fold:  5\n",
      "---------epoch: 0 ---------\n",
      " train loss  0.6853008230737682\n",
      " val loss  0.6715805124027938\n",
      " train accuracy  0.5399493437715864\n",
      " val accuracy  0.5834101382488479\n",
      "------Early stopping after epoch: 9 ---------\n",
      " train loss  0.6187445921418087\n",
      " val loss  0.6814705141129033\n",
      " train accuracy  0.6265254432419987\n",
      " val accuracy  0.5907834101382489\n",
      "cv_score:  0.5644239631336406\n",
      "current setting is  lr0 0.001 layer_num 1 lstm_dim 10 hidden_dim 20 p_drop 0.1\n",
      "current score is 0.5644239631336406\n",
      "best score is 0.5644239631336406\n",
      "best_parameters are {'lr0': 0.001, 'layer_num': 1, 'lstm_dim': 10, 'hidden_dim': 20, 'p_drop': 0.1}\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "k = 5\n",
    "\n",
    "params = {}\n",
    "params['lr0'] = [0.001]\n",
    "\n",
    "params['lstm_dim'] = [10]\n",
    "params['layer_num'] = [1]\n",
    "params['hidden_dim'] = [20]\n",
    "params['p_drop'] = [0.1]\n",
    "\n",
    "result = []\n",
    "\n",
    "best_params = {}\n",
    "best_score = 0\n",
    "for lr0 in params['lr0']:\n",
    "    for layer_num in params['layer_num']:\n",
    "        for lstm_dim in params['lstm_dim']:\n",
    "            for hidden_dim in params['hidden_dim']:\n",
    "                for p_drop in params['p_drop']:\n",
    "                    inputs = rnn_bidirect, init_word_embs, weights, lstm_dim, layer_num, hidden_dim,p_drop\n",
    "                    score = LSTM_attn_kFold(k, epochs,lr0,*inputs)\n",
    "                    result.append(score)\n",
    "\n",
    "                    print('current setting is ','lr0',lr0,'layer_num',layer_num,'lstm_dim',lstm_dim,'hidden_dim',hidden_dim,'p_drop',p_drop)\n",
    "                    print('current score is',score)\n",
    "\n",
    "                    if score>best_score:\n",
    "                        best_score = score\n",
    "                        best_params['lr0'] = lr0\n",
    "                        best_params['layer_num'] = layer_num\n",
    "                        best_params['lstm_dim'] = lstm_dim\n",
    "                        best_params['hidden_dim'] = hidden_dim\n",
    "                        best_params['p_drop'] = p_drop\n",
    "\n",
    "print('best score is', best_score)\n",
    "print('best_parameters are', best_params)\n",
    "                    \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2-layer attention + target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_attn2_target(torch.nn.Module):\n",
    "    def __init__(self, rnn_bidirect, glove, load_weights, lstm_dim=10, layer_num=1, hidden_dim = 20,p_drop=0.1):\n",
    "\n",
    "        # TODO: Declare LSTM model architecture\n",
    "        super(LSTM_attn2_target, self).__init__()\n",
    "\n",
    "        if glove == 'glove':\n",
    "            self.embedding = nn.Embedding.from_pretrained(load_weights, freeze=True)\n",
    "        else:\n",
    "            self.embedding = nn.Embedding.from_pretrained(load_weights, freeze=False)\n",
    "\n",
    "        if rnn_bidirect:\n",
    "            self.lstm = nn.LSTM(50, lstm_dim, num_layers=layer_num, bias=False, bidirectional = True) # one layer, bidirectional\n",
    "            self.hidden_layer = nn.Linear(4*lstm_dim,hidden_dim)\n",
    "            \n",
    "            self.softmax = nn.Softmax(dim=1)\n",
    "            self.ws1 = nn.Linear(2*lstm_dim, 2*lstm_dim, bias=False)\n",
    "            self.ws2 = nn.Linear(2*lstm_dim, 2*lstm_dim, bias=False)\n",
    "\n",
    "        else:\n",
    "            self.lstm = nn.LSTM(50, lstm_dim, num_layers=layer_num, bias=False) # two layers\n",
    "            self.hidden_layer = nn.Linear(2*lstm_dim,hidden_dim)\n",
    "\n",
    "            self.softmax = nn.Softmax(dim=1)\n",
    "            self.ws1 = nn.Linear(lstm_dim, lstm_dim, bias=False)\n",
    "            self.ws2 = nn.Linear(lstm_dim, lstm_dim, bias=False)\n",
    "\n",
    "        # final hidden layer\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=p_drop)\n",
    "        self.output_layer = nn.Linear(hidden_dim, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, s1, s2, idx1, idx2):\n",
    "        # TODO: Implement LSTM forward pass\n",
    "        embed1 = self.embedding(s1).squeeze(0)\n",
    "\t\t# print(\"embedding_squeeze\",self.embedding(s1).squeeze(0).shape)\n",
    "        lstm_out1,(_,_) = self.lstm(embed1)\n",
    "        # print('lstm',lstm_out1.shape)\n",
    "\n",
    "        ws11 = self.ws1(lstm_out1)\n",
    "        # print('ws1',ws1.shape)\n",
    "\n",
    "        similar11 = torch.bmm(ws11.unsqueeze(0), torch.transpose(lstm_out1,0,1).unsqueeze(0))\n",
    "        # print('similar1',similar1.shape)\n",
    "        \n",
    "        attn11 = self.softmax(torch.squeeze(similar11))\n",
    "        # print('attn1',attn1.shape)\n",
    "\n",
    "        context11 = torch.squeeze(torch.bmm(attn11.unsqueeze(0), lstm_out1.unsqueeze(0)))\n",
    "\n",
    "        ws12 = self.ws2(context11)\n",
    "        similar12 = torch.bmm(ws12.unsqueeze(0), torch.transpose(context11,0,1).unsqueeze(0))        \n",
    "        attn12 = self.softmax(torch.squeeze(similar12))\n",
    "        context12 = torch.squeeze(torch.bmm(attn12.unsqueeze(0), context11.unsqueeze(0)))\n",
    "\n",
    "\n",
    "        embed2 = self.embedding(s2).squeeze(0)\n",
    "        lstm_out2,(_,_) = self.lstm(embed2)\n",
    "        \n",
    "        ws21 = self.ws1(lstm_out2)\n",
    "        similar21 = torch.bmm(ws21.unsqueeze(0), torch.transpose(lstm_out2,0,1).unsqueeze(0))        \n",
    "        attn21 = self.softmax(torch.squeeze(similar21))\n",
    "        context21 = torch.squeeze(torch.bmm(attn21.unsqueeze(0), lstm_out2.unsqueeze(0)))\n",
    "\n",
    "        ws22 = self.ws2(context21)\n",
    "        similar22 = torch.bmm(ws22.unsqueeze(0), torch.transpose(context21,0,1).unsqueeze(0))        \n",
    "        attn22 = self.softmax(torch.squeeze(similar22))\n",
    "        context22 = torch.squeeze(torch.bmm(attn22.unsqueeze(0), context21.unsqueeze(0)))\n",
    "\n",
    "        # cat_rep = torch.cat((lstm_out1[-1,:].unsqueeze(0), lstm_out2[-1,:].unsqueeze(0)),1)\n",
    "        try:\n",
    "            cat_rep = torch.cat((context12[idx1,:].unsqueeze(0), context22[idx2,:].unsqueeze(0)),1)\n",
    "            ratio1, ratio2 = attn12[idx1,:], attn22[idx1,:]\n",
    "        except:\n",
    "            cat_rep = torch.cat((context12[-1,:].unsqueeze(0), context22[-1,:].unsqueeze(0)),1)\n",
    "            ratio1, ratio2 = attn12[-1,:], attn22[-1,:]\n",
    "            # in total of 8 wrong indexing samples\n",
    "            # print('wrong index')\n",
    "\n",
    "        hidden_rep = self.hidden_layer(cat_rep)\n",
    "        relu_rep = self.relu(hidden_rep)\n",
    "\t\t# print(\"hidden\",hidden_rep.shape)\n",
    "        drop = self.dropout(relu_rep)\n",
    "\t\t# print(\"drop\",drop.shape)\n",
    "        output = self.output_layer(drop)\n",
    "        # print(\"ouput\",output.shape)\n",
    "\n",
    "        output = self.sigmoid(output)\n",
    "        # print(\"sigmoid\",output.shape)\n",
    "\n",
    "        return output.squeeze(0).squeeze(0), ratio1, ratio2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Training and validation loop here\n",
    "\n",
    "lr0 = 0.001\n",
    "epochs = 20\n",
    "patience = 5\n",
    "\n",
    "def training_attn(model,train_dataset, valid_dataset,lr0,epochs):\n",
    "    ce = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr0)\n",
    "\n",
    "    train_acc = []\n",
    "    dev_acc = []\n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "    # test_output = []\n",
    "\n",
    "    best_val_loss = 1\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        #print(\"Epoch:\",i)\n",
    "        total_loss = 0\n",
    "\n",
    "        for i in range(len(train_dataset)):\n",
    "            sample = train_dataset[i]\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # a) calculate probs / get an output\n",
    "            if init_word_embs == \"glove\":\n",
    "                s1 = sen2glove(sample[\"sentence1\"],glove_embs)\n",
    "                s2 = sen2glove(sample[\"sentence2\"],glove_embs)\n",
    "\n",
    "                # print(sample[\"sentence1\"])\n",
    "                # print(sample[\"sentence2\"])\n",
    "\n",
    "            else:\n",
    "                s1 = sen2vec(sample[\"sentence1\"])\n",
    "                s2 = sen2vec(sample[\"sentence2\"])\n",
    "\n",
    "            # print(int(sample['idx1']))\n",
    "            # print(int(sample['idx2']))\n",
    "\n",
    "            y_raw,_, _ = model(s1,s2,int(sample['idx1']),int(sample['idx2']))\n",
    "            \n",
    "\n",
    "            y = tensor(float(sample[\"label\"]))\n",
    "            \n",
    "            # b) compute loss\n",
    "            loss = ce(y_raw,y)\n",
    "            total_loss += loss\n",
    "\n",
    "            # c) get the gradient\n",
    "            loss.backward()\n",
    "\n",
    "            # d) update the weights\n",
    "            optimizer.step()\n",
    "        train_loss.append(total_loss.item()/len(train_dataset))\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        score = 0\n",
    "        \n",
    "        for i in range(len(train_dataset)):\n",
    "            sample = train_dataset[i]\n",
    "\n",
    "            # a) calculate probs / get an output\n",
    "            if init_word_embs == \"glove\":\n",
    "                s1 = sen2glove(sample[\"sentence1\"],glove_embs)\n",
    "                s2 = sen2glove(sample[\"sentence2\"],glove_embs)\n",
    "            else:\n",
    "                s1 = sen2vec(sample[\"sentence1\"])\n",
    "                s2 = sen2vec(sample[\"sentence2\"])\n",
    "\n",
    "            y_raw,_, _ = model(s1,s2,int(sample['idx1']),int(sample['idx2']))\n",
    "\n",
    "            result = True if y_raw >= 0.5 else False\n",
    "\n",
    "            if bool(result) == sample[\"label\"]:\n",
    "                score += 1\n",
    "        \n",
    "        train_acc.append(score/len(train_dataset))\n",
    "        \n",
    "\n",
    "        score = 0\n",
    "        total_loss = 0\n",
    "        ratios1 = []\n",
    "        ratios2 = []\n",
    "        for i in range(len(valid_dataset)):\n",
    "            sample = valid_dataset[i]\n",
    "            # a) calculate probs / get an output\n",
    "            if init_word_embs == \"glove\":\n",
    "                s1 = sen2glove(sample[\"sentence1\"],glove_embs)\n",
    "                s2 = sen2glove(sample[\"sentence2\"],glove_embs)\n",
    "            else:\n",
    "                s1 = sen2vec(sample[\"sentence1\"])\n",
    "                s2 = sen2vec(sample[\"sentence2\"])\n",
    "\n",
    "            y_raw,ratio1, ratio2 = model(s1,s2,int(sample['idx1']),int(sample['idx2']))\n",
    "            \n",
    "            ratios1.append(ratio1)\n",
    "            ratios2.append(ratio2)\n",
    "            \n",
    "            y = tensor(float(sample[\"label\"]))\n",
    "            loss = ce(y_raw,y)\n",
    "            total_loss += loss\n",
    "\n",
    "            result = True if y_raw >= 0.5 else False\n",
    "            if bool(result) == sample[\"label\"]:\n",
    "                score += 1\n",
    "\n",
    "        val_loss.append(total_loss.item()/len(valid_dataset))\n",
    "        dev_acc.append(score/len(valid_dataset))\n",
    "\n",
    "\n",
    "\n",
    "        if epoch% 10 == 0:\n",
    "            print(\"---------epoch:\",epoch,\"---------\")\n",
    "            print(\" train loss \", train_loss[-1]) \n",
    "            print(\" val loss \", val_loss[-1])\n",
    "            print(\" train accuracy \",train_acc[-1])\n",
    "            print(\" val accuracy \",dev_acc[-1])\n",
    "\n",
    "        # check if validation loss has improvedval loss < best val loss:\n",
    "        if val_loss[-1] < best_val_loss:\n",
    "            best_val_loss = val_loss[-1] \n",
    "            early_stop_counter = 0\n",
    "        else:\n",
    "            early_stop_counter += 1\n",
    "            if early_stop_counter >= patience:\n",
    "                print(\"------Early stopping after epoch:\",epoch,\"---------\")\n",
    "                print(\" train loss \", train_loss[-1]) \n",
    "                print(\" val loss \", val_loss[-1]) \n",
    "                print(\" train accuracy \",train_acc[-1])\n",
    "                print(\" val accuracy \",dev_acc[-1])\n",
    "                return train_loss,val_loss, train_acc,dev_acc, ratios1, ratios2\n",
    "\n",
    "\n",
    "    print(\"---------endng epoch:\",epoch,\"---------\")\n",
    "    print(\" train loss \", train_loss[-1]) \n",
    "    print(\" val loss \", val_loss[-1]) \n",
    "    print(\" train accuracy \",train_acc[-1])\n",
    "    print(\" val accuracy \",dev_acc[-1])\n",
    "    return train_loss,val_loss, train_acc,dev_acc,ratios1, ratios2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------epoch: 0 ---------\n",
      " train loss  0.6810116075643653\n",
      " val loss  0.6977732786937941\n",
      " train accuracy  0.5379513633014001\n",
      " val accuracy  0.512539184952978\n",
      "------Early stopping after epoch: 7 ---------\n",
      " train loss  0.6231228428547577\n",
      " val loss  0.7146856612920014\n",
      " train accuracy  0.6374355195283714\n",
      " val accuracy  0.5235109717868338\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5235109717868338"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr0 = 0.001\n",
    "epochs = 20\n",
    "model = LSTM_attn2_target(rnn_bidirect,init_word_embs, weights).to(torch_device)\n",
    "train_loss,val_loss,train_acc,dev_acc,ratios1, ratios2 = training_attn(model,train_data, dev_data,lr0,epochs)\n",
    "dev_acc[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'board'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_data[0]['word']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2502, 0.2499, 0.2500, 0.2500], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratios1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Room and board .'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_data[0]['sentence1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1539, 0.1954, 0.1412, 0.1365, 0.1261, 0.1249, 0.1221],\n",
       "       grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratios2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'He nailed boards across the windows .'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_data[0]['sentence2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTM_attn2_kFold(k,epochs,lr0,*inputs):\n",
    "    \n",
    "    num_val_samples = len(train_data)//k\n",
    "    cv_score = []\n",
    "    for i in range(k):\n",
    "        print('Processing fold: ', i + 1)\n",
    "        \"\"\"%%%% Initiate new model %%%%\"\"\" #in every fold\n",
    "        model = LSTM_attn2_target(*inputs).to(torch_device)\n",
    "\n",
    "        valid_idx = np.arange(len(train_data))[i * num_val_samples:(i + 1) * num_val_samples]\n",
    "        train_idx = np.concatenate([np.arange(len(train_data))[:i * num_val_samples], np.arange(len(train_data))[(i + 1) * num_val_samples:]], axis=0)\n",
    "        \n",
    "        train_dataset = Subset(train_data, train_idx)\n",
    "        valid_dataset = Subset(train_data, valid_idx)\n",
    "\n",
    "        \n",
    "        _,_,_,valid_acc, _,_ = training_attn(model,train_dataset, valid_dataset,lr0,epochs)\n",
    "        cv_score.append(valid_acc[-1])\n",
    "    \n",
    "    print('cv_score: ',sum(cv_score)/len(cv_score))\n",
    "\n",
    "    return sum(cv_score)/len(cv_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing fold:  1\n",
      "---------epoch: 0 ---------\n",
      " train loss  0.6837330501309579\n",
      " val loss  0.7207113942792339\n",
      " train accuracy  0.5595210683859083\n",
      " val accuracy  0.4626728110599078\n",
      "------Early stopping after epoch: 9 ---------\n",
      " train loss  0.6135031295871229\n",
      " val loss  0.7146558067216302\n",
      " train accuracy  0.6631360810499655\n",
      " val accuracy  0.5447004608294931\n",
      "Processing fold:  2\n",
      "---------epoch: 0 ---------\n",
      " train loss  0.6825090303361732\n",
      " val loss  0.705116208687356\n",
      " train accuracy  0.5369560211835137\n",
      " val accuracy  0.49216589861751153\n",
      "------Early stopping after epoch: 7 ---------\n",
      " train loss  0.6237642875388556\n",
      " val loss  0.7122252574164747\n",
      " train accuracy  0.628137232327884\n",
      " val accuracy  0.5410138248847927\n",
      "Processing fold:  3\n",
      "---------epoch: 0 ---------\n",
      " train loss  0.6793690434859256\n",
      " val loss  0.6962838309151785\n",
      " train accuracy  0.5404098549389823\n",
      " val accuracy  0.5142857142857142\n",
      "------Early stopping after epoch: 9 ---------\n",
      " train loss  0.5977694664906171\n",
      " val loss  0.693665282078053\n",
      " train accuracy  0.6589914805434032\n",
      " val accuracy  0.5815668202764976\n",
      "Processing fold:  4\n",
      "---------epoch: 0 ---------\n",
      " train loss  0.683748902688234\n",
      " val loss  0.674000486031106\n",
      " train accuracy  0.5245222196638268\n",
      " val accuracy  0.5732718894009217\n",
      "------Early stopping after epoch: 8 ---------\n",
      " train loss  0.6146757128928736\n",
      " val loss  0.6887662122875864\n",
      " train accuracy  0.6332028551692378\n",
      " val accuracy  0.5944700460829493\n",
      "Processing fold:  5\n",
      "---------epoch: 0 ---------\n",
      " train loss  0.6838555420540238\n",
      " val loss  0.6675022951468894\n",
      " train accuracy  0.550771356205388\n",
      " val accuracy  0.5686635944700461\n",
      "------Early stopping after epoch: 7 ---------\n",
      " train loss  0.617363958075495\n",
      " val loss  0.6804010769189228\n",
      " train accuracy  0.6000460511167396\n",
      " val accuracy  0.5944700460829493\n",
      "cv_score:  0.5712442396313364\n",
      "current setting is  lr0 0.001 layer_num 1 lstm_dim 10 hidden_dim 20 p_drop 0.1\n",
      "current score is 0.5712442396313364\n",
      "best score is 0.5712442396313364\n",
      "best_parameters are {'lr0': 0.001, 'layer_num': 1, 'lstm_dim': 10, 'hidden_dim': 20, 'p_drop': 0.1}\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "k = 5\n",
    "\n",
    "params = {}\n",
    "params['lr0'] = [0.001]\n",
    "\n",
    "params['lstm_dim'] = [10]\n",
    "params['layer_num'] = [1]\n",
    "params['hidden_dim'] = [20]\n",
    "params['p_drop'] = [0.1]\n",
    "\n",
    "result = []\n",
    "\n",
    "best_params = {}\n",
    "best_score = 0\n",
    "for lr0 in params['lr0']:\n",
    "    for layer_num in params['layer_num']:\n",
    "        for lstm_dim in params['lstm_dim']:\n",
    "            for hidden_dim in params['hidden_dim']:\n",
    "                for p_drop in params['p_drop']:\n",
    "                    inputs = rnn_bidirect, init_word_embs, weights, lstm_dim, layer_num, hidden_dim,p_drop\n",
    "                    score = LSTM_attn2_kFold(k, epochs,lr0,*inputs)\n",
    "                    result.append(score)\n",
    "\n",
    "                    print('current setting is ','lr0',lr0,'layer_num',layer_num,'lstm_dim',lstm_dim,'hidden_dim',hidden_dim,'p_drop',p_drop)\n",
    "                    print('current score is',score)\n",
    "\n",
    "                    if score>best_score:\n",
    "                        best_score = score\n",
    "                        best_params['lr0'] = lr0\n",
    "                        best_params['layer_num'] = layer_num\n",
    "                        best_params['lstm_dim'] = lstm_dim\n",
    "                        best_params['hidden_dim'] = hidden_dim\n",
    "                        best_params['p_drop'] = p_drop\n",
    "\n",
    "print('best score is', best_score)\n",
    "print('best_parameters are', best_params)\n",
    "                    \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2-head attention + target (sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_attn2_target(torch.nn.Module):\n",
    "    def __init__(self, rnn_bidirect, glove, load_weights, lstm_dim=10, layer_num=1, hidden_dim = 20,p_drop=0.1):\n",
    "\n",
    "        # TODO: Declare LSTM model architecture\n",
    "        super(LSTM_attn2_target, self).__init__()\n",
    "\n",
    "        if glove == 'glove':\n",
    "            self.embedding = nn.Embedding.from_pretrained(load_weights, freeze=True)\n",
    "        else:\n",
    "            self.embedding = nn.Embedding.from_pretrained(load_weights, freeze=False)\n",
    "\n",
    "        if rnn_bidirect:\n",
    "            self.lstm = nn.LSTM(50, lstm_dim, num_layers=layer_num, bias=False, bidirectional = True) # one layer, bidirectional\n",
    "            self.hidden_layer = nn.Linear(4*lstm_dim,hidden_dim)\n",
    "            \n",
    "            self.softmax = nn.Softmax(dim=1)\n",
    "            self.ws1 = nn.Linear(2*lstm_dim, 2*lstm_dim, bias=False)\n",
    "            self.ws2 = nn.Linear(2*lstm_dim, 2*lstm_dim, bias=False)\n",
    "\n",
    "        else:\n",
    "            self.lstm = nn.LSTM(50, lstm_dim, num_layers=layer_num, bias=False) # two layers\n",
    "            self.hidden_layer = nn.Linear(2*lstm_dim,hidden_dim)\n",
    "\n",
    "            self.softmax = nn.Softmax(dim=1)\n",
    "            self.ws1 = nn.Linear(lstm_dim, lstm_dim, bias=False)\n",
    "            self.ws2 = nn.Linear(lstm_dim, lstm_dim, bias=False)\n",
    "\n",
    "        # final hidden layer\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=p_drop)\n",
    "        self.output_layer = nn.Linear(hidden_dim, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, s1, s2, idx1, idx2):\n",
    "        # TODO: Implement LSTM forward pass\n",
    "        embed1 = self.embedding(s1).squeeze(0)\n",
    "\t\t# print(\"embedding_squeeze\",self.embedding(s1).squeeze(0).shape)\n",
    "        lstm_out1,(_,_) = self.lstm(embed1)\n",
    "        # print('lstm',lstm_out1.shape)\n",
    "\n",
    "        ws11 = self.ws1(lstm_out1)\n",
    "        # print('ws1',ws1.shape)\n",
    "\n",
    "        similar11 = torch.bmm(ws11.unsqueeze(0), torch.transpose(lstm_out1,0,1).unsqueeze(0))\n",
    "        # print('similar1',similar1.shape)\n",
    "        \n",
    "        attn11 = self.softmax(torch.squeeze(similar11))\n",
    "        # print('attn1',attn1.shape)\n",
    "\n",
    "        context11 = torch.squeeze(torch.bmm(attn11.unsqueeze(0), lstm_out1.unsqueeze(0)))\n",
    "\n",
    "        ws12 = self.ws2(lstm_out1)\n",
    "        similar12 = torch.bmm(ws12.unsqueeze(0), torch.transpose(lstm_out1,0,1).unsqueeze(0))        \n",
    "        attn12 = self.softmax(torch.squeeze(similar12))\n",
    "        context12 = torch.squeeze(torch.bmm(attn12.unsqueeze(0), lstm_out1.unsqueeze(0)))\n",
    "\n",
    "\n",
    "        embed2 = self.embedding(s2).squeeze(0)\n",
    "        lstm_out2,(_,_) = self.lstm(embed2)\n",
    "        \n",
    "        ws21 = self.ws1(lstm_out2)\n",
    "        similar21 = torch.bmm(ws21.unsqueeze(0), torch.transpose(lstm_out2,0,1).unsqueeze(0))        \n",
    "        attn21 = self.softmax(torch.squeeze(similar21))\n",
    "        context21 = torch.squeeze(torch.bmm(attn21.unsqueeze(0), lstm_out2.unsqueeze(0)))\n",
    "\n",
    "        ws22 = self.ws2(lstm_out2)\n",
    "        similar22 = torch.bmm(ws22.unsqueeze(0), torch.transpose(lstm_out2,0,1).unsqueeze(0))        \n",
    "        attn22 = self.softmax(torch.squeeze(similar22))\n",
    "        context22 = torch.squeeze(torch.bmm(attn22.unsqueeze(0), lstm_out2.unsqueeze(0)))\n",
    "\n",
    "        # cat_rep = torch.cat((lstm_out1[-1,:].unsqueeze(0), lstm_out2[-1,:].unsqueeze(0)),1)\n",
    "        try:\n",
    "            cat_rep = torch.cat((context11[idx1,:].unsqueeze(0) + context12[idx1,:].unsqueeze(0), context21[idx2,:].unsqueeze(0) + context22[idx2,:].unsqueeze(0)),1)\n",
    "            ratio1, ratio2 = attn12[idx1,:], attn22[idx1,:]\n",
    "        except:\n",
    "            cat_rep = torch.cat((context11[-1,:].unsqueeze(0) + context12[-1,:].unsqueeze(0), context21[-1,:].unsqueeze(0) + context22[-1,:].unsqueeze(0)),1)\n",
    "            ratio1, ratio2 = attn12[-1,:], attn22[-1,:]\n",
    "            # in total of 8 wrong indexing samples\n",
    "            # print('wrong index')\n",
    "\n",
    "        hidden_rep = self.hidden_layer(cat_rep)\n",
    "        relu_rep = self.relu(hidden_rep)\n",
    "\t\t# print(\"hidden\",hidden_rep.shape)\n",
    "        drop = self.dropout(relu_rep)\n",
    "\t\t# print(\"drop\",drop.shape)\n",
    "        output = self.output_layer(drop)\n",
    "        # print(\"ouput\",output.shape)\n",
    "\n",
    "        output = self.sigmoid(output)\n",
    "        # print(\"sigmoid\",output.shape)\n",
    "\n",
    "        return output.squeeze(0).squeeze(0), ratio1, ratio2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Training and validation loop here\n",
    "\n",
    "lr0 = 0.001\n",
    "epochs = 20\n",
    "patience = 5\n",
    "\n",
    "def training_attn(model,train_dataset, valid_dataset,lr0,epochs):\n",
    "    ce = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr0)\n",
    "\n",
    "    train_acc = []\n",
    "    dev_acc = []\n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "    # test_output = []\n",
    "\n",
    "    best_val_loss = 1\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        #print(\"Epoch:\",i)\n",
    "        total_loss = 0\n",
    "\n",
    "        for i in range(len(train_dataset)):\n",
    "            sample = train_dataset[i]\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # a) calculate probs / get an output\n",
    "            if init_word_embs == \"glove\":\n",
    "                s1 = sen2glove(sample[\"sentence1\"],glove_embs)\n",
    "                s2 = sen2glove(sample[\"sentence2\"],glove_embs)\n",
    "\n",
    "                # print(sample[\"sentence1\"])\n",
    "                # print(sample[\"sentence2\"])\n",
    "\n",
    "            else:\n",
    "                s1 = sen2vec(sample[\"sentence1\"])\n",
    "                s2 = sen2vec(sample[\"sentence2\"])\n",
    "\n",
    "            # print(int(sample['idx1']))\n",
    "            # print(int(sample['idx2']))\n",
    "\n",
    "            y_raw,_, _ = model(s1,s2,int(sample['idx1']),int(sample['idx2']))\n",
    "            \n",
    "\n",
    "            y = tensor(float(sample[\"label\"]))\n",
    "            \n",
    "            # b) compute loss\n",
    "            loss = ce(y_raw,y)\n",
    "            total_loss += loss\n",
    "\n",
    "            # c) get the gradient\n",
    "            loss.backward()\n",
    "\n",
    "            # d) update the weights\n",
    "            optimizer.step()\n",
    "        train_loss.append(total_loss.item()/len(train_dataset))\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        score = 0\n",
    "        \n",
    "        for i in range(len(train_dataset)):\n",
    "            sample = train_dataset[i]\n",
    "\n",
    "            # a) calculate probs / get an output\n",
    "            if init_word_embs == \"glove\":\n",
    "                s1 = sen2glove(sample[\"sentence1\"],glove_embs)\n",
    "                s2 = sen2glove(sample[\"sentence2\"],glove_embs)\n",
    "            else:\n",
    "                s1 = sen2vec(sample[\"sentence1\"])\n",
    "                s2 = sen2vec(sample[\"sentence2\"])\n",
    "\n",
    "            y_raw,_, _ = model(s1,s2,int(sample['idx1']),int(sample['idx2']))\n",
    "\n",
    "            result = True if y_raw >= 0.5 else False\n",
    "\n",
    "            if bool(result) == sample[\"label\"]:\n",
    "                score += 1\n",
    "        \n",
    "        train_acc.append(score/len(train_dataset))\n",
    "        \n",
    "\n",
    "        score = 0\n",
    "        total_loss = 0\n",
    "        ratios1 = []\n",
    "        ratios2 = []\n",
    "        for i in range(len(valid_dataset)):\n",
    "            sample = valid_dataset[i]\n",
    "            # a) calculate probs / get an output\n",
    "            if init_word_embs == \"glove\":\n",
    "                s1 = sen2glove(sample[\"sentence1\"],glove_embs)\n",
    "                s2 = sen2glove(sample[\"sentence2\"],glove_embs)\n",
    "            else:\n",
    "                s1 = sen2vec(sample[\"sentence1\"])\n",
    "                s2 = sen2vec(sample[\"sentence2\"])\n",
    "\n",
    "            y_raw,ratio1, ratio2 = model(s1,s2,int(sample['idx1']),int(sample['idx2']))\n",
    "            \n",
    "            ratios1.append(ratio1)\n",
    "            ratios2.append(ratio2)\n",
    "            \n",
    "            y = tensor(float(sample[\"label\"]))\n",
    "            loss = ce(y_raw,y)\n",
    "            total_loss += loss\n",
    "\n",
    "            result = True if y_raw >= 0.5 else False\n",
    "            if bool(result) == sample[\"label\"]:\n",
    "                score += 1\n",
    "\n",
    "        val_loss.append(total_loss.item()/len(valid_dataset))\n",
    "        dev_acc.append(score/len(valid_dataset))\n",
    "\n",
    "\n",
    "\n",
    "        if epoch% 10 == 0:\n",
    "            print(\"---------epoch:\",epoch,\"---------\")\n",
    "            print(\" train loss \", train_loss[-1]) \n",
    "            print(\" val loss \", val_loss[-1])\n",
    "            print(\" train accuracy \",train_acc[-1])\n",
    "            print(\" val accuracy \",dev_acc[-1])\n",
    "\n",
    "        # check if validation loss has improvedval loss < best val loss:\n",
    "        if val_loss[-1] < best_val_loss:\n",
    "            best_val_loss = val_loss[-1] \n",
    "            early_stop_counter = 0\n",
    "        else:\n",
    "            early_stop_counter += 1\n",
    "            if early_stop_counter >= patience:\n",
    "                print(\"------Early stopping after epoch:\",epoch,\"---------\")\n",
    "                print(\" train loss \", train_loss[-1]) \n",
    "                print(\" val loss \", val_loss[-1]) \n",
    "                print(\" train accuracy \",train_acc[-1])\n",
    "                print(\" val accuracy \",dev_acc[-1])\n",
    "                return train_loss,val_loss, train_acc,dev_acc, ratios1, ratios2\n",
    "\n",
    "\n",
    "    print(\"---------endng epoch:\",epoch,\"---------\")\n",
    "    print(\" train loss \", train_loss[-1]) \n",
    "    print(\" val loss \", val_loss[-1]) \n",
    "    print(\" train accuracy \",train_acc[-1])\n",
    "    print(\" val accuracy \",dev_acc[-1])\n",
    "    return train_loss,val_loss, train_acc,dev_acc,ratios1, ratios2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------epoch: 0 ---------\n",
      " train loss  0.6819875853502672\n",
      " val loss  0.6942834166523805\n",
      " train accuracy  0.5408990420044215\n",
      " val accuracy  0.5188087774294671\n",
      "------Early stopping after epoch: 7 ---------\n",
      " train loss  0.6058645564664702\n",
      " val loss  0.7168540356674912\n",
      " train accuracy  0.6479366249078851\n",
      " val accuracy  0.5297805642633229\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5297805642633229"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr0 = 0.001\n",
    "epochs = 20\n",
    "model = LSTM_attn2_target(rnn_bidirect,init_word_embs, weights).to(torch_device)\n",
    "train_loss,val_loss,train_acc,dev_acc,ratios1, ratios2 = training_attn(model,train_data, dev_data,lr0,epochs)\n",
    "dev_acc[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'board'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_data[0]['word']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2655, 0.2508, 0.2646, 0.2191], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratios1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Room and board .'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_data[0]['sentence1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1507, 0.4432, 0.1769, 0.1183, 0.0509, 0.0306, 0.0294],\n",
       "       grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratios2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'He nailed boards across the windows .'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_data[0]['sentence2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTM_attn2_kFold(k,epochs,lr0,*inputs):\n",
    "    \n",
    "    num_val_samples = len(train_data)//k\n",
    "    cv_score = []\n",
    "    for i in range(k):\n",
    "        print('Processing fold: ', i + 1)\n",
    "        \"\"\"%%%% Initiate new model %%%%\"\"\" #in every fold\n",
    "        model = LSTM_attn2_target(*inputs).to(torch_device)\n",
    "\n",
    "        valid_idx = np.arange(len(train_data))[i * num_val_samples:(i + 1) * num_val_samples]\n",
    "        train_idx = np.concatenate([np.arange(len(train_data))[:i * num_val_samples], np.arange(len(train_data))[(i + 1) * num_val_samples:]], axis=0)\n",
    "        \n",
    "        train_dataset = Subset(train_data, train_idx)\n",
    "        valid_dataset = Subset(train_data, valid_idx)\n",
    "\n",
    "        \n",
    "        _,_,_,valid_acc, _,_ = training_attn(model,train_dataset, valid_dataset,lr0,epochs)\n",
    "        cv_score.append(valid_acc[-1])\n",
    "    \n",
    "    print('cv_score: ',sum(cv_score)/len(cv_score))\n",
    "\n",
    "    return sum(cv_score)/len(cv_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing fold:  1\n",
      "---------epoch: 0 ---------\n",
      " train loss  0.6845903249481925\n",
      " val loss  0.7170086135512673\n",
      " train accuracy  0.5597513239696063\n",
      " val accuracy  0.48202764976958523\n",
      "------Early stopping after epoch: 6 ---------\n",
      " train loss  0.6148619523334964\n",
      " val loss  0.7278208367835541\n",
      " train accuracy  0.6624453142988718\n",
      " val accuracy  0.5299539170506913\n",
      "Processing fold:  2\n",
      "---------epoch: 0 ---------\n",
      " train loss  0.6794381314039835\n",
      " val loss  0.7039046186455933\n",
      " train accuracy  0.5562974902141377\n",
      " val accuracy  0.5142857142857142\n",
      "------Early stopping after epoch: 7 ---------\n",
      " train loss  0.6105448849981292\n",
      " val loss  0.7117853542626729\n",
      " train accuracy  0.6516233018650702\n",
      " val accuracy  0.5631336405529954\n",
      "Processing fold:  3\n",
      "---------epoch: 0 ---------\n",
      " train loss  0.6800806096916014\n",
      " val loss  0.6925905007920506\n",
      " train accuracy  0.5486990559521069\n",
      " val accuracy  0.5428571428571428\n",
      "------Early stopping after epoch: 7 ---------\n",
      " train loss  0.6041916634886599\n",
      " val loss  0.7164956369707661\n",
      " train accuracy  0.642413078517154\n",
      " val accuracy  0.5806451612903226\n",
      "Processing fold:  4\n",
      "---------epoch: 0 ---------\n",
      " train loss  0.6821955207194048\n",
      " val loss  0.6703753915250577\n",
      " train accuracy  0.5374165323509095\n",
      " val accuracy  0.5981566820276498\n",
      "------Early stopping after epoch: 6 ---------\n",
      " train loss  0.6092793225089224\n",
      " val loss  0.6909157186059908\n",
      " train accuracy  0.6684319594750173\n",
      " val accuracy  0.5732718894009217\n",
      "Processing fold:  5\n",
      "---------epoch: 0 ---------\n",
      " train loss  0.6824883995258174\n",
      " val loss  0.6676536173315092\n",
      " train accuracy  0.5569882569652314\n",
      " val accuracy  0.5889400921658986\n",
      "---------epoch: 10 ---------\n",
      " train loss  0.5727269231737854\n",
      " val loss  0.689754869311636\n",
      " train accuracy  0.6518535574487682\n",
      " val accuracy  0.6211981566820276\n",
      "------Early stopping after epoch: 11 ---------\n",
      " train loss  0.5587552549540928\n",
      " val loss  0.6910817792338709\n",
      " train accuracy  0.6576099470412158\n",
      " val accuracy  0.6248847926267281\n",
      "cv_score:  0.5743778801843319\n",
      "current setting is  lr0 0.001 layer_num 1 lstm_dim 10 hidden_dim 20 p_drop 0.1\n",
      "current score is 0.5743778801843319\n",
      "best score is 0.5743778801843319\n",
      "best_parameters are {'lr0': 0.001, 'layer_num': 1, 'lstm_dim': 10, 'hidden_dim': 20, 'p_drop': 0.1}\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "k = 5\n",
    "\n",
    "params = {}\n",
    "params['lr0'] = [0.001]\n",
    "\n",
    "params['lstm_dim'] = [10]\n",
    "params['layer_num'] = [1]\n",
    "params['hidden_dim'] = [20]\n",
    "params['p_drop'] = [0.1]\n",
    "\n",
    "result = []\n",
    "\n",
    "best_params = {}\n",
    "best_score = 0\n",
    "for lr0 in params['lr0']:\n",
    "    for layer_num in params['layer_num']:\n",
    "        for lstm_dim in params['lstm_dim']:\n",
    "            for hidden_dim in params['hidden_dim']:\n",
    "                for p_drop in params['p_drop']:\n",
    "                    inputs = rnn_bidirect, init_word_embs, weights, lstm_dim, layer_num, hidden_dim,p_drop\n",
    "                    score = LSTM_attn2_kFold(k, epochs,lr0,*inputs)\n",
    "                    result.append(score)\n",
    "\n",
    "                    print('current setting is ','lr0',lr0,'layer_num',layer_num,'lstm_dim',lstm_dim,'hidden_dim',hidden_dim,'p_drop',p_drop)\n",
    "                    print('current score is',score)\n",
    "\n",
    "                    if score>best_score:\n",
    "                        best_score = score\n",
    "                        best_params['lr0'] = lr0\n",
    "                        best_params['layer_num'] = layer_num\n",
    "                        best_params['lstm_dim'] = lstm_dim\n",
    "                        best_params['hidden_dim'] = hidden_dim\n",
    "                        best_params['p_drop'] = p_drop\n",
    "\n",
    "print('best score is', best_score)\n",
    "print('best_parameters are', best_params)\n",
    "                    \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2-head attention + target (concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_attn2_target(torch.nn.Module):\n",
    "    def __init__(self, rnn_bidirect, glove, load_weights, lstm_dim=10, layer_num=1, hidden_dim = 20,p_drop=0.1):\n",
    "\n",
    "        # TODO: Declare LSTM model architecture\n",
    "        super(LSTM_attn2_target, self).__init__()\n",
    "\n",
    "        if glove == 'glove':\n",
    "            self.embedding = nn.Embedding.from_pretrained(load_weights, freeze=True)\n",
    "        else:\n",
    "            self.embedding = nn.Embedding.from_pretrained(load_weights, freeze=False)\n",
    "\n",
    "        if rnn_bidirect:\n",
    "            self.lstm = nn.LSTM(50, lstm_dim, num_layers=layer_num, bias=False, bidirectional = True) # one layer, bidirectional\n",
    "                        \n",
    "            self.softmax = nn.Softmax(dim=1)\n",
    "            self.ws1 = nn.Linear(2*lstm_dim, 2*lstm_dim, bias=False)\n",
    "            self.ws2 = nn.Linear(2*lstm_dim, 2*lstm_dim, bias=False)\n",
    "\n",
    "            self.hidden_layer = nn.Linear(2*4*lstm_dim,hidden_dim)\n",
    "\n",
    "        else:\n",
    "            self.lstm = nn.LSTM(50, lstm_dim, num_layers=layer_num, bias=False) # two layers\n",
    "\n",
    "            self.softmax = nn.Softmax(dim=1)\n",
    "            self.ws1 = nn.Linear(lstm_dim, lstm_dim, bias=False)\n",
    "            self.ws2 = nn.Linear(lstm_dim, lstm_dim, bias=False)\n",
    "\n",
    "            self.hidden_layer = nn.Linear(2*2*lstm_dim,hidden_dim)\n",
    "\n",
    "        # final hidden layer\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=p_drop)\n",
    "        self.output_layer = nn.Linear(hidden_dim, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, s1, s2, idx1, idx2):\n",
    "        # TODO: Implement LSTM forward pass\n",
    "        embed1 = self.embedding(s1).squeeze(0)\n",
    "\t\t# print(\"embedding_squeeze\",self.embedding(s1).squeeze(0).shape)\n",
    "        lstm_out1,(_,_) = self.lstm(embed1)\n",
    "        # print('lstm',lstm_out1.shape)\n",
    "\n",
    "        ws11 = self.ws1(lstm_out1)\n",
    "        # print('ws1',ws1.shape)\n",
    "\n",
    "        similar11 = torch.bmm(ws11.unsqueeze(0), torch.transpose(lstm_out1,0,1).unsqueeze(0))\n",
    "        # print('similar1',similar1.shape)\n",
    "        \n",
    "        attn11 = self.softmax(torch.squeeze(similar11))\n",
    "        # print('attn1',attn1.shape)\n",
    "\n",
    "        context11 = torch.squeeze(torch.bmm(attn11.unsqueeze(0), lstm_out1.unsqueeze(0)))\n",
    "\n",
    "        ws12 = self.ws2(lstm_out1)\n",
    "        similar12 = torch.bmm(ws12.unsqueeze(0), torch.transpose(lstm_out1,0,1).unsqueeze(0))        \n",
    "        attn12 = self.softmax(torch.squeeze(similar12))\n",
    "        context12 = torch.squeeze(torch.bmm(attn12.unsqueeze(0), lstm_out1.unsqueeze(0)))\n",
    "\n",
    "\n",
    "        embed2 = self.embedding(s2).squeeze(0)\n",
    "        lstm_out2,(_,_) = self.lstm(embed2)\n",
    "        \n",
    "        ws21 = self.ws1(lstm_out2)\n",
    "        similar21 = torch.bmm(ws21.unsqueeze(0), torch.transpose(lstm_out2,0,1).unsqueeze(0))        \n",
    "        attn21 = self.softmax(torch.squeeze(similar21))\n",
    "        context21 = torch.squeeze(torch.bmm(attn21.unsqueeze(0), lstm_out2.unsqueeze(0)))\n",
    "\n",
    "        ws22 = self.ws2(lstm_out2)\n",
    "        similar22 = torch.bmm(ws22.unsqueeze(0), torch.transpose(lstm_out2,0,1).unsqueeze(0))        \n",
    "        attn22 = self.softmax(torch.squeeze(similar22))\n",
    "        context22 = torch.squeeze(torch.bmm(attn22.unsqueeze(0), lstm_out2.unsqueeze(0)))\n",
    "\n",
    "        # cat_rep = torch.cat((lstm_out1[-1,:].unsqueeze(0), lstm_out2[-1,:].unsqueeze(0)),1)\n",
    "        try:\n",
    "            cat_rep = torch.cat((context11[idx1,:].unsqueeze(0), context12[idx1,:].unsqueeze(0), context21[idx2,:].unsqueeze(0), context22[idx2,:].unsqueeze(0)),1)\n",
    "            ratio1, ratio2 = attn12[idx1,:], attn22[idx1,:]\n",
    "        except:\n",
    "            cat_rep = torch.cat((context11[-1,:].unsqueeze(0), context12[-1,:].unsqueeze(0), context21[-1,:].unsqueeze(0), context22[-1,:].unsqueeze(0)),1)\n",
    "            ratio1, ratio2 = attn12[-1,:], attn22[-1,:]\n",
    "            # in total of 8 wrong indexing samples\n",
    "            # print('wrong index')\n",
    "\n",
    "        hidden_rep = self.hidden_layer(cat_rep)\n",
    "        relu_rep = self.relu(hidden_rep)\n",
    "\t\t# print(\"hidden\",hidden_rep.shape)\n",
    "        drop = self.dropout(relu_rep)\n",
    "\t\t# print(\"drop\",drop.shape)\n",
    "        output = self.output_layer(drop)\n",
    "        # print(\"ouput\",output.shape)\n",
    "\n",
    "        output = self.sigmoid(output)\n",
    "        # print(\"sigmoid\",output.shape)\n",
    "\n",
    "        return output.squeeze(0).squeeze(0), ratio1, ratio2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Training and validation loop here\n",
    "\n",
    "lr0 = 0.001\n",
    "epochs = 20\n",
    "patience = 5\n",
    "\n",
    "def training_attn(model,train_dataset, valid_dataset,lr0,epochs):\n",
    "    ce = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr0)\n",
    "\n",
    "    train_acc = []\n",
    "    dev_acc = []\n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "    # test_output = []\n",
    "\n",
    "    best_val_loss = 1\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        #print(\"Epoch:\",i)\n",
    "        total_loss = 0\n",
    "\n",
    "        for i in range(len(train_dataset)):\n",
    "            sample = train_dataset[i]\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # a) calculate probs / get an output\n",
    "            if init_word_embs == \"glove\":\n",
    "                s1 = sen2glove(sample[\"sentence1\"],glove_embs)\n",
    "                s2 = sen2glove(sample[\"sentence2\"],glove_embs)\n",
    "\n",
    "                # print(sample[\"sentence1\"])\n",
    "                # print(sample[\"sentence2\"])\n",
    "\n",
    "            else:\n",
    "                s1 = sen2vec(sample[\"sentence1\"])\n",
    "                s2 = sen2vec(sample[\"sentence2\"])\n",
    "\n",
    "            # print(int(sample['idx1']))\n",
    "            # print(int(sample['idx2']))\n",
    "\n",
    "            y_raw,_, _ = model(s1,s2,int(sample['idx1']),int(sample['idx2']))\n",
    "            \n",
    "\n",
    "            y = tensor(float(sample[\"label\"]))\n",
    "            \n",
    "            # b) compute loss\n",
    "            loss = ce(y_raw,y)\n",
    "            total_loss += loss\n",
    "\n",
    "            # c) get the gradient\n",
    "            loss.backward()\n",
    "\n",
    "            # d) update the weights\n",
    "            optimizer.step()\n",
    "        train_loss.append(total_loss.item()/len(train_dataset))\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        score = 0\n",
    "        \n",
    "        for i in range(len(train_dataset)):\n",
    "            sample = train_dataset[i]\n",
    "\n",
    "            # a) calculate probs / get an output\n",
    "            if init_word_embs == \"glove\":\n",
    "                s1 = sen2glove(sample[\"sentence1\"],glove_embs)\n",
    "                s2 = sen2glove(sample[\"sentence2\"],glove_embs)\n",
    "            else:\n",
    "                s1 = sen2vec(sample[\"sentence1\"])\n",
    "                s2 = sen2vec(sample[\"sentence2\"])\n",
    "\n",
    "            y_raw,_, _ = model(s1,s2,int(sample['idx1']),int(sample['idx2']))\n",
    "\n",
    "            result = True if y_raw >= 0.5 else False\n",
    "\n",
    "            if bool(result) == sample[\"label\"]:\n",
    "                score += 1\n",
    "        \n",
    "        train_acc.append(score/len(train_dataset))\n",
    "        \n",
    "\n",
    "        score = 0\n",
    "        total_loss = 0\n",
    "        ratios1 = []\n",
    "        ratios2 = []\n",
    "        for i in range(len(valid_dataset)):\n",
    "            sample = valid_dataset[i]\n",
    "            # a) calculate probs / get an output\n",
    "            if init_word_embs == \"glove\":\n",
    "                s1 = sen2glove(sample[\"sentence1\"],glove_embs)\n",
    "                s2 = sen2glove(sample[\"sentence2\"],glove_embs)\n",
    "            else:\n",
    "                s1 = sen2vec(sample[\"sentence1\"])\n",
    "                s2 = sen2vec(sample[\"sentence2\"])\n",
    "\n",
    "            y_raw,ratio1, ratio2 = model(s1,s2,int(sample['idx1']),int(sample['idx2']))\n",
    "            \n",
    "            ratios1.append(ratio1)\n",
    "            ratios2.append(ratio2)\n",
    "            \n",
    "            y = tensor(float(sample[\"label\"]))\n",
    "            loss = ce(y_raw,y)\n",
    "            total_loss += loss\n",
    "\n",
    "            result = True if y_raw >= 0.5 else False\n",
    "            if bool(result) == sample[\"label\"]:\n",
    "                score += 1\n",
    "\n",
    "        val_loss.append(total_loss.item()/len(valid_dataset))\n",
    "        dev_acc.append(score/len(valid_dataset))\n",
    "\n",
    "\n",
    "\n",
    "        if epoch% 10 == 0:\n",
    "            print(\"---------epoch:\",epoch,\"---------\")\n",
    "            print(\" train loss \", train_loss[-1]) \n",
    "            print(\" val loss \", val_loss[-1])\n",
    "            print(\" train accuracy \",train_acc[-1])\n",
    "            print(\" val accuracy \",dev_acc[-1])\n",
    "\n",
    "        # check if validation loss has improvedval loss < best val loss:\n",
    "        if val_loss[-1] < best_val_loss:\n",
    "            best_val_loss = val_loss[-1] \n",
    "            early_stop_counter = 0\n",
    "        else:\n",
    "            early_stop_counter += 1\n",
    "            if early_stop_counter >= patience:\n",
    "                print(\"------Early stopping after epoch:\",epoch,\"---------\")\n",
    "                print(\" train loss \", train_loss[-1]) \n",
    "                print(\" val loss \", val_loss[-1]) \n",
    "                print(\" train accuracy \",train_acc[-1])\n",
    "                print(\" val accuracy \",dev_acc[-1])\n",
    "                return train_loss,val_loss, train_acc,dev_acc, ratios1, ratios2\n",
    "\n",
    "\n",
    "    print(\"---------endng epoch:\",epoch,\"---------\")\n",
    "    print(\" train loss \", train_loss[-1]) \n",
    "    print(\" val loss \", val_loss[-1]) \n",
    "    print(\" train accuracy \",train_acc[-1])\n",
    "    print(\" val accuracy \",dev_acc[-1])\n",
    "    return train_loss,val_loss, train_acc,dev_acc,ratios1, ratios2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------epoch: 0 ---------\n",
      " train loss  0.6816559175225682\n",
      " val loss  0.6878931410260335\n",
      " train accuracy  0.5620854826823877\n",
      " val accuracy  0.5203761755485894\n",
      "------Early stopping after epoch: 7 ---------\n",
      " train loss  0.6037682214904201\n",
      " val loss  0.7159603202604575\n",
      " train accuracy  0.6685703758290347\n",
      " val accuracy  0.5579937304075235\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5579937304075235"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr0 = 0.001\n",
    "epochs = 20\n",
    "model = LSTM_attn2_target(rnn_bidirect,init_word_embs, weights).to(torch_device)\n",
    "train_loss,val_loss,train_acc,dev_acc,ratios1, ratios2 = training_attn(model,train_data, dev_data,lr0,epochs)\n",
    "dev_acc[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'board'"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_data[0]['word']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2518, 0.2513, 0.2304, 0.2665], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratios1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Room and board .'"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_data[0]['sentence1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0083, 0.9028, 0.0624, 0.0143, 0.0030, 0.0048, 0.0045],\n",
       "       grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratios2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'He nailed boards across the windows .'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_data[0]['sentence2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTM_attn2_kFold(k,epochs,lr0,*inputs):\n",
    "    \n",
    "    num_val_samples = len(train_data)//k\n",
    "    cv_score = []\n",
    "    for i in range(k):\n",
    "        print('Processing fold: ', i + 1)\n",
    "        \"\"\"%%%% Initiate new model %%%%\"\"\" #in every fold\n",
    "        model = LSTM_attn2_target(*inputs).to(torch_device)\n",
    "\n",
    "        valid_idx = np.arange(len(train_data))[i * num_val_samples:(i + 1) * num_val_samples]\n",
    "        train_idx = np.concatenate([np.arange(len(train_data))[:i * num_val_samples], np.arange(len(train_data))[(i + 1) * num_val_samples:]], axis=0)\n",
    "        \n",
    "        train_dataset = Subset(train_data, train_idx)\n",
    "        valid_dataset = Subset(train_data, valid_idx)\n",
    "\n",
    "        \n",
    "        _,_,_,valid_acc, _,_ = training_attn(model,train_dataset, valid_dataset,lr0,epochs)\n",
    "        cv_score.append(valid_acc[-1])\n",
    "    \n",
    "    print('cv_score: ',sum(cv_score)/len(cv_score))\n",
    "\n",
    "    return sum(cv_score)/len(cv_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing fold:  1\n",
      "---------epoch: 0 ---------\n",
      " train loss  0.6852519724628713\n",
      " val loss  0.723383046514977\n",
      " train accuracy  0.5537646787934607\n",
      " val accuracy  0.4488479262672811\n",
      "------Early stopping after epoch: 8 ---------\n",
      " train loss  0.6013444992300829\n",
      " val loss  0.7148447063112039\n",
      " train accuracy  0.6783329495740271\n",
      " val accuracy  0.5382488479262673\n",
      "Processing fold:  2\n",
      "---------epoch: 0 ---------\n",
      " train loss  0.6806315141643161\n",
      " val loss  0.7068355437247984\n",
      " train accuracy  0.5493898227032006\n",
      " val accuracy  0.49953917050691243\n",
      "---------epoch: 10 ---------\n",
      " train loss  0.5687789056203949\n",
      " val loss  0.7040557158158122\n",
      " train accuracy  0.6886944508404329\n",
      " val accuracy  0.5751152073732719\n",
      "------Early stopping after epoch: 12 ---------\n",
      " train loss  0.5331153737875605\n",
      " val loss  0.7353909400201613\n",
      " train accuracy  0.7195486990559521\n",
      " val accuracy  0.5806451612903226\n",
      "Processing fold:  3\n",
      "---------epoch: 0 ---------\n",
      " train loss  0.6803192412718743\n",
      " val loss  0.6966179773005472\n",
      " train accuracy  0.5447847110292424\n",
      " val accuracy  0.5271889400921659\n",
      "------Early stopping after epoch: 7 ---------\n",
      " train loss  0.6098631688205157\n",
      " val loss  0.7059084843930011\n",
      " train accuracy  0.6357356665899148\n",
      " val accuracy  0.5557603686635945\n",
      "Processing fold:  4\n",
      "---------epoch: 0 ---------\n",
      " train loss  0.6811541988364898\n",
      " val loss  0.6721407982610887\n",
      " train accuracy  0.5268247755008059\n",
      " val accuracy  0.5907834101382489\n",
      "------Early stopping after epoch: 6 ---------\n",
      " train loss  0.616694271852694\n",
      " val loss  0.6958389211909562\n",
      " train accuracy  0.6495510016117891\n",
      " val accuracy  0.5778801843317972\n",
      "Processing fold:  5\n",
      "---------epoch: 0 ---------\n",
      " train loss  0.6841941234457748\n",
      " val loss  0.6698332755796371\n",
      " train accuracy  0.5088648399723693\n",
      " val accuracy  0.5935483870967742\n",
      "---------epoch: 10 ---------\n",
      " train loss  0.5677706942205849\n",
      " val loss  0.6790293328773042\n",
      " train accuracy  0.6384987335942897\n",
      " val accuracy  0.6248847926267281\n",
      "------Early stopping after epoch: 11 ---------\n",
      " train loss  0.5547331463705963\n",
      " val loss  0.6820321500576036\n",
      " train accuracy  0.6355054110062169\n",
      " val accuracy  0.6294930875576037\n",
      "cv_score:  0.5764055299539171\n",
      "current setting is  lr0 0.001 layer_num 1 lstm_dim 10 hidden_dim 20 p_drop 0.1\n",
      "current score is 0.5764055299539171\n",
      "best score is 0.5764055299539171\n",
      "best_parameters are {'lr0': 0.001, 'layer_num': 1, 'lstm_dim': 10, 'hidden_dim': 20, 'p_drop': 0.1}\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "k = 5\n",
    "\n",
    "params = {}\n",
    "params['lr0'] = [0.001]\n",
    "\n",
    "params['lstm_dim'] = [10]\n",
    "params['layer_num'] = [1]\n",
    "params['hidden_dim'] = [20]\n",
    "params['p_drop'] = [0.1]\n",
    "\n",
    "result = []\n",
    "\n",
    "best_params = {}\n",
    "best_score = 0\n",
    "for lr0 in params['lr0']:\n",
    "    for layer_num in params['layer_num']:\n",
    "        for lstm_dim in params['lstm_dim']:\n",
    "            for hidden_dim in params['hidden_dim']:\n",
    "                for p_drop in params['p_drop']:\n",
    "                    inputs = rnn_bidirect, init_word_embs, weights, lstm_dim, layer_num, hidden_dim,p_drop\n",
    "                    score = LSTM_attn2_kFold(k, epochs,lr0,*inputs)\n",
    "                    result.append(score)\n",
    "\n",
    "                    print('current setting is ','lr0',lr0,'layer_num',layer_num,'lstm_dim',lstm_dim,'hidden_dim',hidden_dim,'p_drop',p_drop)\n",
    "                    print('current score is',score)\n",
    "\n",
    "                    if score>best_score:\n",
    "                        best_score = score\n",
    "                        best_params['lr0'] = lr0\n",
    "                        best_params['layer_num'] = layer_num\n",
    "                        best_params['lstm_dim'] = lstm_dim\n",
    "                        best_params['hidden_dim'] = hidden_dim\n",
    "                        best_params['p_drop'] = p_drop\n",
    "\n",
    "print('best score is', best_score)\n",
    "print('best_parameters are', best_params)\n",
    "                    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1-layer multi-head attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_attn_target(torch.nn.Module):\n",
    "    def __init__(self, rnn_bidirect, glove, load_weights, lstm_dim=10, layer_num=1, hidden_dim = 20,p_drop=0.1):\n",
    "\n",
    "        # TODO: Declare LSTM model architecture\n",
    "        super(LSTM_attn_target, self).__init__()\n",
    "\n",
    "        if glove == 'glove':\n",
    "            self.embedding = nn.Embedding.from_pretrained(load_weights, freeze=True)\n",
    "        else:\n",
    "            self.embedding = nn.Embedding.from_pretrained(load_weights, freeze=False)\n",
    "\n",
    "        if rnn_bidirect:\n",
    "            self.lstm = nn.LSTM(50, lstm_dim, num_layers=layer_num, bias=False, bidirectional = True) # one layer, bidirectional\n",
    "            self.multihead = nn.MultiheadAttention(2*lstm_dim, 1)\n",
    "            \n",
    "            self.hidden_layer = nn.Linear(4*lstm_dim,hidden_dim)\n",
    "            \n",
    "            # self.softmax = nn.Softmax(dim=0)\n",
    "            # self.ws = nn.Linear(2*lstm_dim, 2*lstm_dim, bias=False)\n",
    "\n",
    "        else:\n",
    "            self.lstm = nn.LSTM(50, lstm_dim, num_layers=layer_num, bias=False) # two layers\n",
    "            self.multihead = nn.MultiheadAttention(lstm_dim, 1)\n",
    "\n",
    "            self.hidden_layer = nn.Linear(2*lstm_dim,hidden_dim)\n",
    "\n",
    "            # self.softmax = nn.Softmax(dim=0)\n",
    "            # self.ws = nn.Linear(lstm_dim, lstm_dim, bias=False)\n",
    "\n",
    "        # final hidden layer\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=p_drop)\n",
    "        self.output_layer = nn.Linear(hidden_dim, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, s1, s2, idx1, idx2):\n",
    "        # TODO: Implement LSTM forward pass\n",
    "        embed1 = self.embedding(s1).squeeze(0)\n",
    "\t\t# print(\"embedding_squeeze\",self.embedding(s1).squeeze(0).shape)\n",
    "        lstm_out1,(_,_) = self.lstm(embed1)\n",
    "        # print('lstm',lstm_out1.shape)\n",
    "\n",
    "        context1,weights1 = self.multihead(lstm_out1, lstm_out1, lstm_out1)\n",
    "        # print('context1',context1.shape)\n",
    "\n",
    "        # condense1 = self.condense(context1)\n",
    "        # print('condense1',condense1.shape)\n",
    "\n",
    "        embed2 = self.embedding(s2).squeeze(0)\n",
    "        lstm_out2,(_,_) = self.lstm(embed2)\n",
    "        \n",
    "        context2, weights2 = self.multihead(lstm_out2, lstm_out2, lstm_out2)\n",
    "\n",
    "        # cat_rep = torch.cat((lstm_out1[-1,:].unsqueeze(0), lstm_out2[-1,:].unsqueeze(0)),1)\n",
    "        try:\n",
    "            cat_rep = torch.cat((context1[idx1,:].unsqueeze(0), context2[idx2,:].unsqueeze(0)),1)\n",
    "            ratio1, ratio2 = weights1[idx1,:], weights2[idx2,:]\n",
    "        except:\n",
    "            cat_rep = torch.cat((context1[-1,:].unsqueeze(0), context2[-1,:].unsqueeze(0)),1)\n",
    "            ratio1, ratio2 = weights1[-1,:], weights2[-1,:]\n",
    "            # in total of 8 wrong indexing samples\n",
    "            # print('wrong index')\n",
    "\n",
    "\n",
    "\n",
    "        hidden_rep = self.hidden_layer(cat_rep)\n",
    "        relu_rep = self.relu(hidden_rep)\n",
    "\t\t# print(\"hidden\",hidden_rep.shape)\n",
    "        drop = self.dropout(relu_rep)\n",
    "\t\t# print(\"drop\",drop.shape)\n",
    "        output = self.output_layer(drop)\n",
    "        # print(\"ouput\",output.shape)\n",
    "\n",
    "        output = self.sigmoid(output)\n",
    "        # print(\"sigmoid\",output.shape)\n",
    "\n",
    "        return output.squeeze(0).squeeze(0),ratio1, ratio2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Training and validation loop here\n",
    "\n",
    "lr0 = 0.001\n",
    "epochs = 20\n",
    "patience = 5\n",
    "\n",
    "def training_attn(model,train_dataset, valid_dataset,lr0,epochs):\n",
    "    ce = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr0)\n",
    "\n",
    "    train_acc = []\n",
    "    dev_acc = []\n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "    # test_output = []\n",
    "\n",
    "    best_val_loss = 1\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        #print(\"Epoch:\",i)\n",
    "        total_loss = 0\n",
    "\n",
    "        for i in range(len(train_dataset)):\n",
    "            sample = train_dataset[i]\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # a) calculate probs / get an output\n",
    "            if init_word_embs == \"glove\":\n",
    "                s1 = sen2glove(sample[\"sentence1\"],glove_embs)\n",
    "                s2 = sen2glove(sample[\"sentence2\"],glove_embs)\n",
    "\n",
    "                # print(sample[\"sentence1\"])\n",
    "                # print(sample[\"sentence2\"])\n",
    "\n",
    "            else:\n",
    "                s1 = sen2vec(sample[\"sentence1\"])\n",
    "                s2 = sen2vec(sample[\"sentence2\"])\n",
    "\n",
    "            # print(int(sample['idx1']))\n",
    "            # print(int(sample['idx2']))\n",
    "\n",
    "            y_raw,_, _ = model(s1,s2,int(sample['idx1']),int(sample['idx2']))\n",
    "            \n",
    "\n",
    "            y = tensor(float(sample[\"label\"]))\n",
    "            \n",
    "            # b) compute loss\n",
    "            loss = ce(y_raw,y)\n",
    "            total_loss += loss\n",
    "\n",
    "            # c) get the gradient\n",
    "            loss.backward()\n",
    "\n",
    "            # d) update the weights\n",
    "            optimizer.step()\n",
    "        train_loss.append(total_loss.item()/len(train_dataset))\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        score = 0\n",
    "        \n",
    "        for i in range(len(train_dataset)):\n",
    "            sample = train_dataset[i]\n",
    "\n",
    "            # a) calculate probs / get an output\n",
    "            if init_word_embs == \"glove\":\n",
    "                s1 = sen2glove(sample[\"sentence1\"],glove_embs)\n",
    "                s2 = sen2glove(sample[\"sentence2\"],glove_embs)\n",
    "            else:\n",
    "                s1 = sen2vec(sample[\"sentence1\"])\n",
    "                s2 = sen2vec(sample[\"sentence2\"])\n",
    "\n",
    "            y_raw,_, _ = model(s1,s2,int(sample['idx1']),int(sample['idx2']))\n",
    "\n",
    "            result = True if y_raw >= 0.5 else False\n",
    "\n",
    "            if bool(result) == sample[\"label\"]:\n",
    "                score += 1\n",
    "        \n",
    "        train_acc.append(score/len(train_dataset))\n",
    "        \n",
    "\n",
    "        score = 0\n",
    "        total_loss = 0\n",
    "        ratios1 = []\n",
    "        ratios2 = []\n",
    "        for i in range(len(valid_dataset)):\n",
    "            sample = valid_dataset[i]\n",
    "            # a) calculate probs / get an output\n",
    "            if init_word_embs == \"glove\":\n",
    "                s1 = sen2glove(sample[\"sentence1\"],glove_embs)\n",
    "                s2 = sen2glove(sample[\"sentence2\"],glove_embs)\n",
    "            else:\n",
    "                s1 = sen2vec(sample[\"sentence1\"])\n",
    "                s2 = sen2vec(sample[\"sentence2\"])\n",
    "\n",
    "            y_raw,ratio1, ratio2 = model(s1,s2,int(sample['idx1']),int(sample['idx2']))\n",
    "            \n",
    "            ratios1.append(ratio1)\n",
    "            ratios2.append(ratio2)\n",
    "            \n",
    "            y = tensor(float(sample[\"label\"]))\n",
    "            loss = ce(y_raw,y)\n",
    "            total_loss += loss\n",
    "\n",
    "            result = True if y_raw >= 0.5 else False\n",
    "            if bool(result) == sample[\"label\"]:\n",
    "                score += 1\n",
    "\n",
    "        val_loss.append(total_loss.item()/len(valid_dataset))\n",
    "        dev_acc.append(score/len(valid_dataset))\n",
    "\n",
    "\n",
    "\n",
    "        if epoch% 10 == 0:\n",
    "            print(\"---------epoch:\",epoch,\"---------\")\n",
    "            print(\" train loss \", train_loss[-1]) \n",
    "            print(\" val loss \", val_loss[-1])\n",
    "            print(\" train accuracy \",train_acc[-1])\n",
    "            print(\" val accuracy \",dev_acc[-1])\n",
    "\n",
    "        # check if validation loss has improvedval loss < best val loss:\n",
    "        if val_loss[-1] < best_val_loss:\n",
    "            best_val_loss = val_loss[-1] \n",
    "            early_stop_counter = 0\n",
    "        else:\n",
    "            early_stop_counter += 1\n",
    "            if early_stop_counter >= patience:\n",
    "                print(\"------Early stopping after epoch:\",epoch,\"---------\")\n",
    "                print(\" train loss \", train_loss[-1]) \n",
    "                print(\" val loss \", val_loss[-1]) \n",
    "                print(\" train accuracy \",train_acc[-1])\n",
    "                print(\" val accuracy \",dev_acc[-1])\n",
    "                return train_loss,val_loss, train_acc,dev_acc, ratios1, ratios2\n",
    "\n",
    "\n",
    "    print(\"---------endng epoch:\",epoch,\"---------\")\n",
    "    print(\" train loss \", train_loss[-1]) \n",
    "    print(\" val loss \", val_loss[-1]) \n",
    "    print(\" train accuracy \",train_acc[-1])\n",
    "    print(\" val accuracy \",dev_acc[-1])\n",
    "    return train_loss,val_loss, train_acc,dev_acc,ratios1, ratios2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------epoch: 0 ---------\n",
      " train loss  0.6826701266148903\n",
      " val loss  0.698405155193843\n",
      " train accuracy  0.5373986735445836\n",
      " val accuracy  0.5031347962382445\n",
      "------Early stopping after epoch: 5 ---------\n",
      " train loss  0.6369852896727616\n",
      " val loss  0.719779035514425\n",
      " train accuracy  0.6254605747973471\n",
      " val accuracy  0.5250783699059561\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5250783699059561"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr0 = 0.001\n",
    "epochs = 20\n",
    "model = LSTM_attn_target(rnn_bidirect,init_word_embs, weights).to(torch_device)\n",
    "train_loss,val_loss,train_acc,dev_acc,ratios1, ratios2 = training_attn(model,train_data, dev_data,lr0,epochs)\n",
    "dev_acc[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'board'"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_data[0]['word']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2572, 0.2505, 0.2476, 0.2448], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratios1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Room and board .'"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_data[0]['sentence1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0253, 0.7720, 0.0816, 0.0693, 0.0182, 0.0160, 0.0176],\n",
       "       grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratios2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'He nailed boards across the windows .'"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_data[0]['sentence2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTM_attn_kFold(k,epochs,lr0,*inputs):\n",
    "    \n",
    "    num_val_samples = len(train_data)//k\n",
    "    cv_score = []\n",
    "    for i in range(k):\n",
    "        print('Processing fold: ', i + 1)\n",
    "        \"\"\"%%%% Initiate new model %%%%\"\"\" #in every fold\n",
    "        model = LSTM_attn_target(*inputs).to(torch_device)\n",
    "\n",
    "        valid_idx = np.arange(len(train_data))[i * num_val_samples:(i + 1) * num_val_samples]\n",
    "        train_idx = np.concatenate([np.arange(len(train_data))[:i * num_val_samples], np.arange(len(train_data))[(i + 1) * num_val_samples:]], axis=0)\n",
    "        \n",
    "        train_dataset = Subset(train_data, train_idx)\n",
    "        valid_dataset = Subset(train_data, valid_idx)\n",
    "\n",
    "        \n",
    "        _,_,_,valid_acc, _,_ = training_attn(model,train_dataset, valid_dataset,lr0,epochs)\n",
    "        cv_score.append(valid_acc[-1])\n",
    "    \n",
    "    print('cv_score: ',sum(cv_score)/len(cv_score))\n",
    "\n",
    "    return sum(cv_score)/len(cv_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing fold:  1\n",
      "---------epoch: 0 ---------\n",
      " train loss  0.6852999798526365\n",
      " val loss  0.7107114730342742\n",
      " train accuracy  0.5691918029012204\n",
      " val accuracy  0.5013824884792627\n",
      "---------epoch: 10 ---------\n",
      " train loss  0.5840586346743323\n",
      " val loss  0.7394361926663306\n",
      " train accuracy  0.6758001381533503\n",
      " val accuracy  0.5188940092165899\n",
      "------Early stopping after epoch: 10 ---------\n",
      " train loss  0.5840586346743323\n",
      " val loss  0.7394361926663306\n",
      " train accuracy  0.6758001381533503\n",
      " val accuracy  0.5188940092165899\n",
      "Processing fold:  2\n",
      "---------epoch: 0 ---------\n",
      " train loss  0.6802585293503914\n",
      " val loss  0.7049229775705645\n",
      " train accuracy  0.5629749021413769\n",
      " val accuracy  0.5271889400921659\n",
      "---------epoch: 10 ---------\n",
      " train loss  0.5728284469980428\n",
      " val loss  0.7314239361319125\n",
      " train accuracy  0.700437485609026\n",
      " val accuracy  0.5585253456221199\n",
      "------Early stopping after epoch: 10 ---------\n",
      " train loss  0.5728284469980428\n",
      " val loss  0.7314239361319125\n",
      " train accuracy  0.700437485609026\n",
      " val accuracy  0.5585253456221199\n",
      "Processing fold:  3\n",
      "---------epoch: 0 ---------\n",
      " train loss  0.6806245997510362\n",
      " val loss  0.6915854177167339\n",
      " train accuracy  0.5496200782868984\n",
      " val accuracy  0.5271889400921659\n",
      "---------epoch: 10 ---------\n",
      " train loss  0.5890256568580187\n",
      " val loss  0.6933914958057316\n",
      " train accuracy  0.6483997236932996\n",
      " val accuracy  0.5751152073732719\n",
      "------Early stopping after epoch: 11 ---------\n",
      " train loss  0.5772486683851888\n",
      " val loss  0.7075932234663018\n",
      " train accuracy  0.6617545475477781\n",
      " val accuracy  0.5714285714285714\n",
      "Processing fold:  4\n",
      "---------epoch: 0 ---------\n",
      " train loss  0.683789939449977\n",
      " val loss  0.670377191640265\n",
      " train accuracy  0.5325811650932535\n",
      " val accuracy  0.5953917050691244\n",
      "------Early stopping after epoch: 8 ---------\n",
      " train loss  0.5956717009591584\n",
      " val loss  0.6950059178787442\n",
      " train accuracy  0.6707345153119963\n",
      " val accuracy  0.5834101382488479\n",
      "Processing fold:  5\n",
      "---------epoch: 0 ---------\n",
      " train loss  0.6849958580978011\n",
      " val loss  0.667427252844182\n",
      " train accuracy  0.5454754777803362\n",
      " val accuracy  0.5834101382488479\n",
      "------Early stopping after epoch: 9 ---------\n",
      " train loss  0.5897611143290928\n",
      " val loss  0.678868166312644\n",
      " train accuracy  0.6412618005986646\n",
      " val accuracy  0.6101382488479262\n",
      "cv_score:  0.5684792626728111\n",
      "current setting is  lr0 0.001 layer_num 1 lstm_dim 10 hidden_dim 20 p_drop 0.1\n",
      "current score is 0.5684792626728111\n",
      "best score is 0.5684792626728111\n",
      "best_parameters are {'lr0': 0.001, 'layer_num': 1, 'lstm_dim': 10, 'hidden_dim': 20, 'p_drop': 0.1}\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "k = 5\n",
    "\n",
    "params = {}\n",
    "params['lr0'] = [0.001]\n",
    "\n",
    "params['lstm_dim'] = [10]\n",
    "params['layer_num'] = [1]\n",
    "params['hidden_dim'] = [20]\n",
    "params['p_drop'] = [0.1]\n",
    "\n",
    "result = []\n",
    "\n",
    "best_params = {}\n",
    "best_score = 0\n",
    "for lr0 in params['lr0']:\n",
    "    for layer_num in params['layer_num']:\n",
    "        for lstm_dim in params['lstm_dim']:\n",
    "            for hidden_dim in params['hidden_dim']:\n",
    "                for p_drop in params['p_drop']:\n",
    "                    inputs = rnn_bidirect, init_word_embs, weights, lstm_dim, layer_num, hidden_dim,p_drop\n",
    "                    score = LSTM_attn_kFold(k, epochs,lr0,*inputs)\n",
    "                    result.append(score)\n",
    "\n",
    "                    print('current setting is ','lr0',lr0,'layer_num',layer_num,'lstm_dim',lstm_dim,'hidden_dim',hidden_dim,'p_drop',p_drop)\n",
    "                    print('current score is',score)\n",
    "\n",
    "                    if score>best_score:\n",
    "                        best_score = score\n",
    "                        best_params['lr0'] = lr0\n",
    "                        best_params['layer_num'] = layer_num\n",
    "                        best_params['lstm_dim'] = lstm_dim\n",
    "                        best_params['hidden_dim'] = hidden_dim\n",
    "                        best_params['p_drop'] = p_drop\n",
    "\n",
    "print('best score is', best_score)\n",
    "print('best_parameters are', best_params)\n",
    "                    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Testing loop\n",
    "# Write predictions (F or T) for each test example into test.pred.txt\n",
    "# One line per each example, in the same order as test.data.txt.\n",
    "\n",
    "# score = 0\n",
    "# for i in range(len(test_data)):\n",
    "#     sample = test_data[i]\n",
    "#     # a) calculate probs / get an output\n",
    "#     if init_word_embs == \"glove\":\n",
    "#         s1 = sen2glove(sample[\"sentence1\"],glove_embs)\n",
    "#         s2 = sen2glove(sample[\"sentence2\"],glove_embs)\n",
    "#     else:\n",
    "#         s1 = sen2vec(sample[\"sentence1\"])\n",
    "#         s2 = sen2vec(sample[\"sentence2\"])\n",
    "\n",
    "#     y_raw = model(s1,s2)\n",
    "#     result = True if y_raw >= 0.5 else False\n",
    "#     if bool(result) == sample[\"label\"]:\n",
    "#         score += 1\n",
    "\n",
    "#     output = \"T\" if y_raw >= 0.5 else \"F\"\n",
    "#     test_output.append(result)\n",
    "\n",
    "# print(\" test accuracy \",score/len(test_data))\n",
    "\n",
    "# with open('test.pred.txt', 'w') as f:\n",
    "#     for line in test_output:\n",
    "#         f.write(f\"{line}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
